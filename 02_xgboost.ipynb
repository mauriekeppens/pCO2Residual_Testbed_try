{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4647fbee-e926-411f-b0b1-4c8910a09fe9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-30 21:43:47.259169: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-12-30 21:43:47.270471: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-12-30 21:43:47.284131: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-12-30 21:43:47.288276: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-12-30 21:43:47.298558: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# standard imports\n",
    "import os\n",
    "import datetime\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "import scipy\n",
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import glob\n",
    "pd.set_option('display.max_colwidth',100)\n",
    "import joblib\n",
    "import pickle\n",
    "import csv\n",
    "\n",
    "# machine learning libraries\n",
    "import sklearn            # machine-learning libary with many algorithms implemented\n",
    "from sklearn.metrics import make_scorer\n",
    "import xgboost as xgb     # extreme gradient boosting (XGB)\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Python file with supporting functions\n",
    "import residual_utils as supporting_functions\n",
    "\n",
    "import gcsfs\n",
    "fs = gcsfs.GCSFileSystem()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d01a373b-88c7-472c-b92e-cc5120d49c7d",
   "metadata": {},
   "source": [
    "<span style=\"color:hotpink; font-size:40px; font-weight:bold;\">Setting date range</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5f6e3009-8569-441b-bbd6-fe703b5297f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define date range\n",
    "date_range_start = '1982-02-01T00:00:00.000000000'\n",
    "date_range_end = '2023-12-31T00:00:00.000000000'\n",
    "\n",
    "# create date vector, adds 14 days to start & end\n",
    "dates = pd.date_range(start=date_range_start, \n",
    "                      end=date_range_end,freq='MS')\n",
    "\n",
    "init_date = str(dates[0].year) + format(dates[0].month,'02d')\n",
    "fin_date = str(dates[-1].year) + format(dates[-1].month,'02d')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f3404c8-8def-4597-b83f-e775744958ab",
   "metadata": {},
   "source": [
    "<span style=\"color:hotpink; font-size:40px; font-weight:bold;\">Setting paths and choosing grid search approach</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cf5cbc3-c215-456f-99ac-ef9748dd4892",
   "metadata": {},
   "source": [
    "<span style=\"color:lightblue; font-size:30px; font-weight:bold;\">Grid search approach</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "231fff3a-5f02-4a47-a363-b28cb21426de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for grid search, we can specify the metric to optimize for\n",
    "# the options we can currently choose between are 'nmse' (negative mean square error) and 'bias' \n",
    "\n",
    "### DEFINE APPROACH HERE: ###\n",
    "grid_search_approach = 'nmse'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f5678ff-9325-491d-a6f6-194ff04f16bf",
   "metadata": {},
   "source": [
    "<span style=\"color:lightblue; font-size:30px; font-weight:bold;\">Paths</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75ef517f-cf44-4120-b171-cf9e19d6d7de",
   "metadata": {},
   "outputs": [],
   "source": [
    "### set paths ###\n",
    "\n",
    "your_username = # leap pangeo username, for bucket. should be your github username\n",
    "\n",
    "### paths for loading: ###\n",
    "\n",
    "# where machine learning inputs are saved\n",
    "MLinputs_path = f\"gs://leap-persistent/{your_username}/pco2_residual/post01_xgb_inputs\"\n",
    "\n",
    "path_seeds = \"gs://leap-persistent/abbysh/pickles/random_seeds.npy\" # random seeds for ML\n",
    "\n",
    "#########################################\n",
    "\n",
    "### paths for saving: ###\n",
    "\n",
    "output_dir = f'gs://leap-persistent/{your_username}/pco2_residual/{grid_search_approach}/post02_xgb' # where to save machine learning results\n",
    "\n",
    "model_output_dir = f\"{output_dir}/trained\" # where to save ML models\n",
    "recon_output_dir = f\"{output_dir}/reconstructions\" # where to save ML reconstructions\n",
    "\n",
    "metrics_output_dir = f'{output_dir}/metrics' # where to save performance metrics\n",
    "test_perform_fname = f\"{metrics_output_dir}/xgb_test_performance_{init_date}-{fin_date}.csv\" # path for test performance metrics\n",
    "unseen_perform_fname = f\"{metrics_output_dir}/xgb_unseen_performance_{init_date}-{fin_date}.csv\" # path for unseen performance metrics\n",
    "\n",
    "xgb_model_save_dir = f'{output_dir}/saved_models_{init_date}-{fin_date}' # where to save .json model file\n",
    "\n",
    "#########################################\n",
    "jobs = 30"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a6a90c3-1b00-4180-bcb2-4e3ad5caebcc",
   "metadata": {},
   "source": [
    "<span style=\"color:hotpink; font-size:40px; font-weight:bold;\">Hyperparameter selection</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "12c5c53e-b32f-44b9-b069-338778da7ba6",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "### Alternative Parameter grids, wider and more zoomed in ###\n",
    "\n",
    "# stage one for wider grid:\n",
    "# xg_param_grid = {\"n_estimators\":[50, 200, 500],\n",
    "#                  \"max_depth\":[4, 10, None],\n",
    "#                  \"\"\n",
    "#                 }\n",
    "\n",
    "# stage two zooming in\n",
    "# xg_param_grid = {\"n_estimators\":[50, 500, 1000, 4000],\n",
    "#                  \"max_depth\":[6, 7, 10, 15],\n",
    "#                  \"learning_rate\":[0.05, 0.10, 0.30]\n",
    "#                 }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cde658f7-032a-4cee-9c85-9c071b190683",
   "metadata": {},
   "outputs": [],
   "source": [
    "# these are the parameters to try for grid search. These are all tried in combination with each other\n",
    "\n",
    "xg_param_grid = {\"n_estimators\":[500, 1000, 2000, 4000],\n",
    "                 \"max_depth\":[6, 7, 10, 15],\n",
    "                 \"learning_rate\":[0.05, 0.10, 0.30, 0.40]\n",
    "                }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "983d14c8-aac6-4f06-99ec-a801348b2908",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you already know what ML parameters you want to use, define them in the dictionary \"best_params_dict\" below:\n",
    "\n",
    "# best_params_dict = {\"n_estimators\": 4000,\n",
    "#                  \"max_depth\": 6,\n",
    "#                  \"learning_rate\": 0.30\n",
    "#                 }\n",
    "\n",
    "# if you've done grid search and the results are saved in a pickle file, and you want to use the results directly from the pickle, use \"best_params_dict = None\"\n",
    "# the path is below in the XGBoost cell\n",
    "\n",
    "best_params_dict = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e83a7c8-5581-4ecf-b2a8-6e7682f961ff",
   "metadata": {},
   "source": [
    "<span style=\"color:lightblue; font-size:30px; font-weight:bold;\">To optimize for bias</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8ee43512-99bd-43dc-9964-1d9e1d90e7b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if optimizing for bias, this is the function to score for low absolute value of bias.\n",
    "# if optimizing for negative mean square error, this is not necessary \n",
    "\n",
    "def bias_fxn(truth,pred):\n",
    "    bias = pred - truth\n",
    "    return np.abs(np.nanmean(bias))\n",
    "\n",
    "bias_scorer = make_scorer(bias_fxn, greater_is_better=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78d4366f-a48d-49c7-a95c-ad424d261abe",
   "metadata": {},
   "source": [
    "<span style=\"color:hotpink; font-size:40px; font-weight:bold;\">Loading list of ESMs and members in testbed</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51eb6c2b-77c9-47cd-bac3-a05b5e3d302b",
   "metadata": {},
   "source": [
    "<span style=\"color:lightblue; font-size:30px; font-weight:bold;\">For all members for each ESM</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c6ee86a9-7d50-4b42-9e23-3250bb1e3d62",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "b/leap-persistent/o/abbysh%2Fpco2_all_members_198202-202312%2Fpost01_xgb_inputs",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m### loads list of Earth System Models (\"ensembles\") and members for the full testbed ###\u001b[39;00m\n\u001b[1;32m      3\u001b[0m ensembles \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m path \u001b[38;5;129;01min\u001b[39;00m \u001b[43mfs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mls\u001b[49m\u001b[43m(\u001b[49m\u001b[43mMLinputs_path\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m      5\u001b[0m     ens \u001b[38;5;241m=\u001b[39m path\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ens \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ensembles:\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.12/site-packages/fsspec/asyn.py:118\u001b[0m, in \u001b[0;36msync_wrapper.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    117\u001b[0m     \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m obj \u001b[38;5;129;01mor\u001b[39;00m args[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m--> 118\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msync\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.12/site-packages/fsspec/asyn.py:103\u001b[0m, in \u001b[0;36msync\u001b[0;34m(loop, func, timeout, *args, **kwargs)\u001b[0m\n\u001b[1;32m    101\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m FSTimeoutError \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mreturn_result\u001b[39;00m\n\u001b[1;32m    102\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(return_result, \u001b[38;5;167;01mBaseException\u001b[39;00m):\n\u001b[0;32m--> 103\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m return_result\n\u001b[1;32m    104\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    105\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m return_result\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.12/site-packages/fsspec/asyn.py:56\u001b[0m, in \u001b[0;36m_runner\u001b[0;34m(event, coro, result, timeout)\u001b[0m\n\u001b[1;32m     54\u001b[0m     coro \u001b[38;5;241m=\u001b[39m asyncio\u001b[38;5;241m.\u001b[39mwait_for(coro, timeout\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 56\u001b[0m     result[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m coro\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m ex:\n\u001b[1;32m     58\u001b[0m     result[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m=\u001b[39m ex\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.12/site-packages/gcsfs/core.py:1030\u001b[0m, in \u001b[0;36mGCSFileSystem._ls\u001b[0;34m(self, path, detail, prefix, versions, refresh, **kwargs)\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1029\u001b[0m     out \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m-> 1030\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m entry \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_list_objects(\n\u001b[1;32m   1031\u001b[0m         path, prefix\u001b[38;5;241m=\u001b[39mprefix, versions\u001b[38;5;241m=\u001b[39mversions, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m   1032\u001b[0m     ):\n\u001b[1;32m   1033\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m versions \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgeneration\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m entry:\n\u001b[1;32m   1034\u001b[0m             entry \u001b[38;5;241m=\u001b[39m entry\u001b[38;5;241m.\u001b[39mcopy()\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.12/site-packages/gcsfs/core.py:591\u001b[0m, in \u001b[0;36mGCSFileSystem._list_objects\u001b[0;34m(self, path, prefix, versions, **kwargs)\u001b[0m\n\u001b[1;32m    589\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (items \u001b[38;5;241m+\u001b[39m pseudodirs):\n\u001b[1;32m    590\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m key:\n\u001b[0;32m--> 591\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_object(path)]\n\u001b[1;32m    592\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    593\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m []\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.12/site-packages/gcsfs/core.py:527\u001b[0m, in \u001b[0;36mGCSFileSystem._get_object\u001b[0;34m(self, path)\u001b[0m\n\u001b[1;32m    524\u001b[0m \u001b[38;5;66;03m# Work around various permission settings. Prefer an object get (storage.objects.get), but\u001b[39;00m\n\u001b[1;32m    525\u001b[0m \u001b[38;5;66;03m# fall back to a bucket list + filter to object name (storage.objects.list).\u001b[39;00m\n\u001b[1;32m    526\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 527\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(\n\u001b[1;32m    528\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGET\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb/\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m/o/\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, bucket, key, json_out\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, generation\u001b[38;5;241m=\u001b[39mgeneration\n\u001b[1;32m    529\u001b[0m     )\n\u001b[1;32m    530\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    531\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(e)\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mForbidden\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.12/site-packages/gcsfs/core.py:447\u001b[0m, in \u001b[0;36mGCSFileSystem._call\u001b[0;34m(self, method, path, json_out, info_out, *args, **kwargs)\u001b[0m\n\u001b[1;32m    443\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_call\u001b[39m(\n\u001b[1;32m    444\u001b[0m     \u001b[38;5;28mself\u001b[39m, method, path, \u001b[38;5;241m*\u001b[39margs, json_out\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, info_out\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    445\u001b[0m ):\n\u001b[1;32m    446\u001b[0m     logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmethod\u001b[38;5;241m.\u001b[39mupper()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00margs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mheaders\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 447\u001b[0m     status, headers, info, contents \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request(\n\u001b[1;32m    448\u001b[0m         method, path, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    449\u001b[0m     )\n\u001b[1;32m    450\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m json_out:\n\u001b[1;32m    451\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m json\u001b[38;5;241m.\u001b[39mloads(contents)\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.12/site-packages/decorator.py:221\u001b[0m, in \u001b[0;36mdecorate.<locals>.fun\u001b[0;34m(*args, **kw)\u001b[0m\n\u001b[1;32m    219\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kwsyntax:\n\u001b[1;32m    220\u001b[0m     args, kw \u001b[38;5;241m=\u001b[39m fix(args, kw, sig)\n\u001b[0;32m--> 221\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m caller(func, \u001b[38;5;241m*\u001b[39m(extras \u001b[38;5;241m+\u001b[39m args), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.12/site-packages/gcsfs/retry.py:130\u001b[0m, in \u001b[0;36mretry_request\u001b[0;34m(func, retries, *args, **kwargs)\u001b[0m\n\u001b[1;32m    128\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m retry \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    129\u001b[0m         \u001b[38;5;28;01mawait\u001b[39;00m asyncio\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;28mmin\u001b[39m(random\u001b[38;5;241m.\u001b[39mrandom() \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m (retry \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m), \u001b[38;5;241m32\u001b[39m))\n\u001b[0;32m--> 130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    131\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\n\u001b[1;32m    132\u001b[0m     HttpError,\n\u001b[1;32m    133\u001b[0m     requests\u001b[38;5;241m.\u001b[39mexceptions\u001b[38;5;241m.\u001b[39mRequestException,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    136\u001b[0m     aiohttp\u001b[38;5;241m.\u001b[39mclient_exceptions\u001b[38;5;241m.\u001b[39mClientError,\n\u001b[1;32m    137\u001b[0m ) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    138\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    139\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(e, HttpError)\n\u001b[1;32m    140\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m e\u001b[38;5;241m.\u001b[39mcode \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m400\u001b[39m\n\u001b[1;32m    141\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrequester pays\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m e\u001b[38;5;241m.\u001b[39mmessage\n\u001b[1;32m    142\u001b[0m     ):\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.12/site-packages/gcsfs/core.py:440\u001b[0m, in \u001b[0;36mGCSFileSystem._request\u001b[0;34m(self, method, path, headers, json, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m    437\u001b[0m info \u001b[38;5;241m=\u001b[39m r\u001b[38;5;241m.\u001b[39mrequest_info  \u001b[38;5;66;03m# for debug only\u001b[39;00m\n\u001b[1;32m    438\u001b[0m contents \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m r\u001b[38;5;241m.\u001b[39mread()\n\u001b[0;32m--> 440\u001b[0m \u001b[43mvalidate_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstatus\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontents\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    441\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m status, headers, info, contents\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.12/site-packages/gcsfs/retry.py:95\u001b[0m, in \u001b[0;36mvalidate_response\u001b[0;34m(status, content, path, args)\u001b[0m\n\u001b[1;32m     93\u001b[0m     path \u001b[38;5;241m=\u001b[39m path\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;241m*\u001b[39m[quote(p) \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m args])\n\u001b[1;32m     94\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m status \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m404\u001b[39m:\n\u001b[0;32m---> 95\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(path)\n\u001b[1;32m     97\u001b[0m error \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(content, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdecode\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: b/leap-persistent/o/abbysh%2Fpco2_all_members_198202-202312%2Fpost01_xgb_inputs"
     ]
    }
   ],
   "source": [
    "### loads list of Earth System Models (\"ensembles\") and members for the full testbed ###\n",
    "\n",
    "ensembles = []\n",
    "for path in fs.ls(MLinputs_path):\n",
    "    ens = path.split('/')[-1].split('.')[0]\n",
    "    if ens not in ensembles:\n",
    "        ensembles.append(ens)\n",
    "\n",
    "mems_dict = dict()\n",
    "a = fs.ls(MLinputs_path)\n",
    "for ens_path in a:\n",
    "    ens = ens_path.split('/')[-1]\n",
    "    mems = fs.ls(ens_path)\n",
    "    for mem in mems:\n",
    "        memo = mem.split('/')[-1]\n",
    "        \n",
    "        if ens not in mems_dict:\n",
    "            mems_dict[ens] = [memo]\n",
    "\n",
    "        elif ens in mems_dict:\n",
    "            mems_dict[ens].append(memo)\n",
    "\n",
    "### loads random seeds for ML ###\n",
    "\n",
    "random_seeds = np.load(fs.open(path_seeds))   \n",
    "\n",
    "seed_loc_dict = defaultdict(dict)\n",
    "for ens,mem_list in mems_dict.items():\n",
    "    sub_dictt = dict()\n",
    "    for no,mem in enumerate(mem_list):\n",
    "        sub_dictt.update({mem:no})\n",
    "    seed_loc_dict.update({ens:sub_dictt})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95e0fafc-a8ea-4689-aa0c-a48f0bd6d781",
   "metadata": {},
   "source": [
    "<span style=\"color:lightblue; font-size:30px; font-weight:bold;\">For one member per ESM</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d9a441a-e38f-491d-b4aa-1e4f175097e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "### if you only want one member per model ###\n",
    "\n",
    "ensembles = []\n",
    "for path in fs.ls(old_ensemble_dir):\n",
    "    ens = path.split('/')[-1].split('.')[0]\n",
    "    if ens not in ensembles:\n",
    "        ensembles.append(ens)\n",
    "\n",
    "mems_dict = dict()\n",
    "a = fs.ls(old_ensemble_dir)\n",
    "for ens_path in a:\n",
    "    ens = ens_path.split('/')[-1]\n",
    "    mems = fs.ls(ens_path)\n",
    "    \n",
    "    ### difference is here, only add one member per ensemble to dictionary\n",
    "    for mem in mems[0:1]:\n",
    "        memo = mem.split('/')[-1]\n",
    "        \n",
    "        if ens not in mems_dict:\n",
    "            mems_dict[ens] = [memo]\n",
    "\n",
    "        elif ens in mems_dict:\n",
    "            mems_dict[ens].append(memo)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be932ec2-3c74-403d-9056-269a3032511e",
   "metadata": {},
   "source": [
    "<span style=\"color:hotpink; font-size:40px; font-weight:bold;\">Specifiying feature and target variables</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d0746bd4-0cbe-438b-bea0-dbee75430652",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Feature and target variables for use in the ML model ###\n",
    "\n",
    "# features for ML:\n",
    "features_sel = ['sst','sst_anom','sss','sss_anom','mld_clim_log','chl_log','chl_log_anom','xco2','A', 'B', 'C', 'T0', 'T1']\n",
    "\n",
    "# the target variable we reconstruct:\n",
    "target_sel = ['pco2_residual'] # this represents pCO2 - pCO2_T (calculated in notebook 00)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1d9286f-5247-4764-8b57-76e0fd5d6c21",
   "metadata": {},
   "source": [
    "<span style=\"color:hotpink; font-size:40px; font-weight:bold;\">Isolating training, testing, and validation sets</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "806dd9fc-98c4-4d38-8d8c-cbc5246779b1",
   "metadata": {},
   "source": [
    "We want to make sure we don't use the same data for training as we do for testing.\n",
    "We also want to make sure that we don't use the same months from every year for either testing or training. \n",
    "For example, we don't want the month of May from each year to always be used for testing.\n",
    "\n",
    "Therefore, the way we isolate the training and test sets is as follows:\n",
    "Let's say we are using a testbed from January 1982 through December 2023. \n",
    "This is 504 months in total. Below, the code is counting 0 through 504.\n",
    "If the number that the counting loop is at is divisible by 5, the associated month is used as a testing month. Otherwise, that month is used for training.\n",
    "As a result, for each year in the testbed, two or three months will be used for testing and the rest for training. But, the specific months vary per year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4efeb1e6-3133-4357-8558-500ed254063f",
   "metadata": {},
   "outputs": [],
   "source": [
    "### train-validate-test split proportions ###\n",
    "val_prop = .2 # 20% of data for validation\n",
    "test_prop = .2 # 20% of data for testing\n",
    "\n",
    "select_dates = []\n",
    "test_dates = []\n",
    "\n",
    "for i in range(0,len(dates)):\n",
    "    \n",
    "    if i % 5 != 0:\n",
    "        select_dates.append(dates[i]) ### train days set ###\n",
    "    if i % 5 == 0:\n",
    "        test_dates.append(dates[i]) ### test days set ### \n",
    "\n",
    "### Then, the month numbers above are converted back to their respective datetime objects.\n",
    "\n",
    "year_mon = []\n",
    "\n",
    "for i in range(0,len(select_dates)):\n",
    "    \n",
    "    tmp = select_dates[i]\n",
    "    year_mon.append(f\"{tmp.year}-{tmp.month}\")\n",
    "    \n",
    "test_year_mon = []\n",
    "\n",
    "for i in range(0,len(test_dates)):\n",
    "    \n",
    "    tmp = test_dates[i]\n",
    "    test_year_mon.append(f\"{tmp.year}-{tmp.month}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd7d8a61-89e9-4cb8-84d2-552cd9f7b307",
   "metadata": {},
   "source": [
    "<span style=\"color:hotpink; font-size:40px; font-weight:bold;\">Machine learning happens here</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc9a2539-d618-4ead-8788-ca52b05fe164",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-11-21 04:29:12.044935\n",
      "ACCESS-ESM1-5 member_r4i1p1f1\n",
      "238718\n",
      "58855\n",
      "/home/jovyan/grid_search_test/saved_models_1982-2022_nmse_test/xg_best_params_dict_1982-2022_ACCESS-ESM1-5_member_r4i1p1f1.pickle\n",
      "{'learning_rate': 0.05, 'max_depth': 10, 'n_estimators': 4000}\n",
      "Starting model saving process\n",
      "Save complete\n",
      "test performance metrics: {'mse': 69.45649136902385, 'mae': 5.293047913915207, 'medae': 3.4573132106062303, 'max_error': 172.88574063996077, 'bias': 0.16168219666709582, 'r2': 0.9293837437086994, 'corr': 0.9647763243382422, 'cent_rmse': 8.332487629497209, 'stdev': 29.09101, 'amp_ratio': 0.7955313918087857, 'stdev_ref': 31.362023475346902, 'range_ref': 445.85411982281875, 'iqr_ref': 35.39009281133522}\n",
      "unseen performance metrics: {'mse': 120.11119129551156, 'mae': 6.917735108534175, 'medae': 4.585168425708957, 'max_error': 265.2407128060157, 'bias': 0.9918781548554838, 'r2': 0.8751175797253312, 'corr': 0.93693370755052, 'cent_rmse': 10.914548443128798, 'stdev': 27.776644, 'amp_ratio': 0.8888965109032655, 'stdev_ref': 31.012807521660378, 'range_ref': 499.6785690718387, 'iqr_ref': 38.86167681217614}\n",
      "Starting reconstruction saving process\n",
      "/home/jovyan/pco2_residual_through2023/post02_xgb_1982-2023/reconstructions/ACCESS-ESM1-5/member_r4i1p1f1/recon_pC02residual_ACCESS-ESM1-5_member_r4i1p1f1_mon_1x1_198202_202312.zarr\n",
      "Save complete\n",
      "end of member 2024-11-21 04:36:36.256600\n",
      "ACCESS-ESM1-5 member_r5i1p1f1\n",
      "238758\n",
      "58867\n",
      "/home/jovyan/grid_search_test/saved_models_1982-2022_nmse_test/xg_best_params_dict_1982-2022_ACCESS-ESM1-5_member_r4i1p1f1.pickle\n",
      "{'learning_rate': 0.05, 'max_depth': 10, 'n_estimators': 4000}\n",
      "Starting model saving process\n",
      "Save complete\n",
      "test performance metrics: {'mse': 59.74242359647672, 'mae': 4.932140104911187, 'medae': 3.315842096489291, 'max_error': 182.4172593931006, 'bias': 0.09310281024101652, 'r2': 0.9318922585267697, 'corr': 0.9657649510691462, 'cent_rmse': 7.7287615661806885, 'stdev': 27.765606, 'amp_ratio': 0.6034514518107523, 'stdev_ref': 29.617144142129014, 'range_ref': 445.0125295505422, 'iqr_ref': 34.875438800250265}\n"
     ]
    }
   ],
   "source": [
    "### \"k_folds\" means number of times to do cross validation. \n",
    "### When k_folds = 3, the same parameters are run through grid search 3 times, optimizing for your chosen approach (nmse or bias).\n",
    "### The three scores are then averaged to get the overall score/performance for that combination of hyperparameters.\n",
    "\n",
    "k_folds = 3\n",
    "\n",
    "print(datetime.datetime.now())\n",
    "\n",
    "for ens, mem_list in mems_dict.items():\n",
    "    \n",
    "    # If \"first_mem = True\", grid search will run for one member of each Earth System Model in the testbed.\n",
    "    # If \"first_mem = False\", grid search will not run at all. \n",
    "        # Hyperparameters are then either: specified above in this notebook, or \n",
    "        # retrieved from a previously-made pickle file resulting from grid search.\n",
    "    first_mem = True \n",
    "\n",
    "    for member in mem_list:\n",
    "        print(ens,member)\n",
    "        \n",
    "        seed_loc = seed_loc_dict[ens][member]\n",
    "    \n",
    "        ### ML input path, a dataframe made in notebook 01 ###\n",
    "        data_dir = f\"{MLinputs_path}/{ens}/{member}\"\n",
    "        fname = f\"MLinput_{ens}_{member.split('_')[-1]}_mon_1x1_{init_date}_{fin_date}.pkl\"\n",
    "        file_path = f\"{data_dir}/{fname}\"\n",
    "\n",
    "        ### Read in ML input dataframe, create some selection filters, produce a reduced dataframe ###   \n",
    "        with fs.open(file_path,'rb') as filee:\n",
    "            df = pd.read_pickle(filee)\n",
    "            df['year'] = df.index.get_level_values('time').year\n",
    "            df['mon'] = df.index.get_level_values('time').month\n",
    "            df['year_month'] = df['year'].astype(str) + \"-\" + df['mon'].astype(str)\n",
    "            \n",
    "            ### selects pCO2-residual values that correspond to SOCAT locations, \n",
    "            ### filtered to only use values between -250 and 250 microatm\n",
    "            ### this more or less corresponds to filtering OUT values of pCO2 that are above 800 microatm, and therefore not realistic\n",
    "            recon_sel = (~df[features_sel+target_sel+['net_mask']].isna().any(axis=1)) & ((df[target_sel] < 250) & (df[target_sel] > -250)).to_numpy().ravel()\n",
    "            sel = (recon_sel & ((df['socat_mask'] == 1)))\n",
    "            \n",
    "            ### selection of training data, using filtered selection above, following train/test selection in cell above\n",
    "            train_sel = (sel & (pd.Series(df['year_month']).isin(year_mon))).to_numpy().ravel()\n",
    "            print('Number of training points:',sum(train_sel)) \n",
    "\n",
    "            ### selection of test data, using filtered selection above, following train/test selection in cell above\n",
    "            test_sel = (sel & (pd.Series(df['year_month']).isin(test_year_mon))).to_numpy().ravel()\n",
    "            print('Number of testing points:',sum(test_sel))\n",
    "\n",
    "            ### selection of \"unseen\" data, corresponding to spatiotemporal points NOT in SOCAT (everything a \"0\" in the SOCAT mask)\n",
    "            unseen_sel = (recon_sel & (df['socat_mask'] == 0))\n",
    "\n",
    "            ### Convert dataframe of training and test data to numpy arrays\n",
    "            X = df.loc[sel,features_sel].to_numpy()\n",
    "            y = df.loc[sel,target_sel].to_numpy().ravel()\n",
    "\n",
    "            ### Selects features and target variables from the training set\n",
    "            Xtrain = df.loc[train_sel,features_sel].to_numpy()                \n",
    "            ytrain = df.loc[train_sel,target_sel].to_numpy().ravel()\n",
    "\n",
    "            ### Selects features and target variables from the test set\n",
    "            X_test = df.loc[test_sel,features_sel].to_numpy()\n",
    "            y_test = df.loc[test_sel,target_sel].to_numpy().ravel()  \n",
    "\n",
    "            ### Applying splits in randomized order to training/test/validation sets using supporting python file (\"supporting_functions\")\n",
    "            N = Xtrain.shape[0]\n",
    "            train_val_idx, train_idx, val_idx, test_idx = supporting_functions.train_val_test_split(N, test_prop, val_prop, random_seeds, seed_loc)\n",
    "            X_train_val, X_train, X_val, X_test_tmp, y_train_val, y_train, y_val, y_test_tmp = supporting_functions.apply_splits(Xtrain, ytrain, train_val_idx, train_idx, val_idx, test_idx)   \n",
    "\n",
    "            \n",
    "            ### GRID SEARCH STARTS HERE:\n",
    "            ### Will optimize according to specified approach (bias or nmse)\n",
    "            if first_mem:\n",
    "                model = XGBRegressor(random_state=random_seeds[4,seed_loc], n_jobs=jobs)\n",
    "                param_grid = xg_param_grid\n",
    "\n",
    "                ### bias optimization\n",
    "                if grid_search_approach == 'bias':\n",
    "                    grid = GridSearchCV(model, param_grid, scoring=bias_scorer, cv=k_folds, return_train_score=True, refit=False, \n",
    "                            verbose=3)\n",
    "                    \n",
    "                ### nmse optimization\n",
    "                elif grid_search_approach == 'nmse':\n",
    "                    grid = GridSearchCV(model, param_grid, scoring='neg_mean_squared_error', cv=k_folds, return_train_score=True, refit=False, \n",
    "                                    verbose=3)\n",
    "                print(grid)\n",
    "                ### Grid search here, using features and target variables from both training and validation sets, \n",
    "                    ### so should be using 80% of SOCAT sampling (60% train + 20% validation)\n",
    "                grid.fit(X_train_val, y_train_val) \n",
    "                best_params = pd.DataFrame([grid.best_params_]) # best parameters chosen by grid search function due to best score\n",
    "                print(best_params)\n",
    "                scores = pd.DataFrame(grid.cv_results_)\n",
    "\n",
    "                ### save grid search scores\n",
    "                output_file = f\"CVgrid_scores_{ens}_{member}.csv\"\n",
    "                output_dir = Path(f'{metrics_output_dir}')\n",
    "                output_dir.mkdir(parents=True, exist_ok=True)\n",
    "                scores.to_csv(output_dir / output_file)\n",
    "\n",
    "                param_fname = f\"{metrics_output_dir}/{grid_search_approach}_best_params_dict_{init_date}-{fin_date}_{ens}.pickle\"\n",
    "\n",
    "                ### saving grid search best parameters in pickle file\n",
    "                best_params.to_pickle(param_fname)\n",
    "\n",
    "                ### Here, \"first_mem\" is set to false so grid search stops after one member per ESM\n",
    "                ### Do not change this to True UNLESS you want to do grid search on every member of the testbed\n",
    "                first_mem = False\n",
    "            ###### GRID SEARCH ENDS HERE ######\n",
    "            \n",
    "            ### make blank dictionaries for performance metrics for training/testing/unseen sets\n",
    "            train_performance = defaultdict(dict)\n",
    "            test_performance = defaultdict(dict)\n",
    "            unseen_performance = defaultdict(dict)\n",
    "\n",
    "            ### loading best hyperparameters from up above in notebook, if hyperparameters are hard-coded in notebook\n",
    "            if best_params_dict is not None:\n",
    "                best_params = best_params_dict\n",
    "\n",
    "            ### loading previously-saved best hyperparameters from pickle file post-grid search\n",
    "            elif best_params_dict is None:\n",
    "                best_params_path = f\"{metrics_output_dir}/{grid_search_approach}_best_params_dict_{init_date}-{fin_date}_{ens}.pickle\"\n",
    "                print(best_params_path)\n",
    "                best_params = pd.read_pickle(best_params_path).to_dict('records')[0]\n",
    "                print(best_params)\n",
    "\n",
    "            ### Running ML using hyperparameters: setting up ML model and then training it (\".fit\") on training set\n",
    "            model = XGBRegressor(random_state=random_seeds[5,seed_loc], **best_params, n_jobs=jobs)\n",
    "            model.fit(X_train_val, y_train_val)          \n",
    "            ### Save the resulting ML model\n",
    "            supporting_functions.save_model(model, dates, xgb_model_save_dir, ens, member)\n",
    "            \n",
    "            ### Makes ML prediction/reconstruction on test set ML model to test set (\".predict\" and \".evaluate_test\"), calculate test error metrics and store in a dictionary.\n",
    "            ### X_test is the feature variables in the test set.\n",
    "            ### Using the test set feature variables and the algorithm produced using the training data, \n",
    "                ### pCO2-residual (the target) is predicted.\n",
    "            ### The predicted pCO2-residual within the test set domain is now compared to the TRUE test set pCO2-residual values.\n",
    "            ### The result is the test performance.\n",
    "            y_pred_test = model.predict(X_test)\n",
    "            test_performance[ens][member] = supporting_functions.evaluate_test(y_test, y_pred_test)\n",
    "\n",
    "            ### This saves the test performance metrics for every member in the testbed ###\n",
    "            fields = test_performance[ens][member].keys()\n",
    "            test_row_dict = dict()\n",
    "            test_row_dict['model'] = ens\n",
    "            test_row_dict['member'] = member\n",
    "            \n",
    "            for field in fields:\n",
    "                test_row_dict[field] = test_performance[ens][member][field]\n",
    "\n",
    "            file_exists = os.path.isfile(test_perform_fname)\n",
    "            \n",
    "            with open(test_perform_fname, 'a') as f_object:\n",
    "                writer = csv.DictWriter(f_object, fieldnames = test_row_dict.keys())\n",
    "                if not file_exists:\n",
    "                    writer.writeheader() \n",
    "                writer.writerow(test_row_dict)\n",
    "\n",
    "            print('test performance metrics:', test_performance[ens][member])\n",
    "            \n",
    "            ##############\n",
    "\n",
    "            ### Global reconstruction where we don't have SOCAT data\n",
    "            ### Then compares reconstruction to the testbed truth, which in our case is the pCO2-residual calculated from ESM output\n",
    "            y_pred_unseen = model.predict(df.loc[unseen_sel,features_sel].to_numpy())\n",
    "            y_unseen = df.loc[unseen_sel,target_sel].to_numpy().ravel()\n",
    "            unseen_performance[ens][member] = supporting_functions.evaluate_test(y_unseen, y_pred_unseen)\n",
    "\n",
    "            ### This saves the UNSEEN performance metrics for every member in the testbed ###\n",
    "            fields = unseen_performance[ens][member].keys()\n",
    "            unseen_row_dict = dict()\n",
    "            unseen_row_dict['model'] = ens\n",
    "            unseen_row_dict['member'] = member\n",
    "            \n",
    "            for field in fields:\n",
    "                unseen_row_dict[field] = unseen_performance[ens][member][field]\n",
    "\n",
    "            file_exists = os.path.isfile(unseen_perform_fname)\n",
    "\n",
    "            with open(unseen_perform_fname, 'a') as f_object:\n",
    "                writer = csv.DictWriter(f_object, fieldnames = unseen_row_dict.keys())\n",
    "                if not file_exists:\n",
    "                    writer.writeheader() \n",
    "                writer.writerow(unseen_row_dict)\n",
    "\n",
    "            print('unseen performance metrics:', unseen_performance[ens][member])\n",
    "            \n",
    "            ##############\n",
    "            \n",
    "            # ML model predicts using feature variables from within SOCAT domain (should be all training and test locations/times)\n",
    "            # This is in addition to the unseen prediction that's performed above\n",
    "            y_pred_seen = model.predict(X)\n",
    "\n",
    "            # Full reconstruction globally, every grid cell of testbed #\n",
    "            # Unseen and \"seen\" (train + test) reconstructions are combined\n",
    "            df['pCO2_recon_full'] = np.nan\n",
    "            df.loc[unseen_sel,['pCO2_recon_full']] = y_pred_unseen \n",
    "            df.loc[sel,['pCO2_recon_full']] = y_pred_seen\n",
    "            \n",
    "            # Reconstruction in times/locations of TEST set domain, done by making all TRAINING points into nans\n",
    "            df['pCO2_recon_test'] = np.nan\n",
    "            df.loc[unseen_sel,['pCO2_recon_test']] = np.nan\n",
    "            df.loc[sel,['pCO2_recon_test']] = y_pred_seen\n",
    "            df.loc[train_sel, ['pCO2_recon_test']] = np.nan\n",
    "\n",
    "            # Reconstructed in times/locations of TRAINING set domain, done by making all TESTING points into nans\n",
    "            df['pCO2_recon_train'] = np.nan\n",
    "            df.loc[unseen_sel,['pCO2_recon_train']] = np.nan\n",
    "            df.loc[sel,['pCO2_recon_train']] = y_pred_seen\n",
    "            df.loc[test_sel, ['pCO2_recon_train']] = np.nan\n",
    "        \n",
    "            # Reconstruction everywhere outside of SOCAT domain (this is what we call \"unseen\")\n",
    "            df['pCO2_recon_unseen'] = np.nan\n",
    "            df.loc[unseen_sel,['pCO2_recon_unseen']] = y_pred_unseen\n",
    "            df.loc[sel,['pCO2_recon_unseen']] = np.nan\n",
    "            \n",
    "            # Reconstruction at time/locations of SOCAT sampling (train + test)\n",
    "            df['pCO2_recon_socat'] = np.nan\n",
    "            df.loc[unseen_sel,['pCO2_recon_socat']] = np.nan\n",
    "            df.loc[sel,['pCO2_recon_socat']] = y_pred_seen\n",
    "\n",
    "            # Testbed truth (pco2 residual calculated from ESM output)\n",
    "            df['pCO2_residual_testbed_truth'] = df['pco2_residual']\n",
    "\n",
    "            DS_recon = df[['net_mask','socat_mask','pCO2_residual_testbed_truth','pCO2_recon_full','pCO2_recon_socat','pCO2_recon_unseen', 'pCO2_recon_test', 'pCO2_recon_train']].to_xarray()\n",
    "\n",
    "            supporting_functions.save_recon(DS_recon, dates, recon_output_dir, ens, member)\n",
    "            print('end of member', datetime.datetime.now())\n",
    "            \n",
    "print('end of all members', datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25c6a7eb-3e77-4187-a43e-1cac54bb5052",
   "metadata": {},
   "source": [
    "<span style=\"color:hotpink; font-size:40px; font-weight:bold;\">To view ranking of hyperparameters and their various scores</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa24d1c1-9a14-4e3e-9cbf-bd4a19efb288",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# scoresCV = scores[['params', 'mean_test_score', 'std_test_score', 'mean_train_score']].sort_values(by = 'mean_test_score' \\\n",
    "#                    , ascending = False) #mean = averaged over the k folds (in this case, the 3 partitions of the cross validation); std = standard deviation\n",
    "\n",
    "# scoresCV = scores[['params', 'mean_test_score', 'std_test_score', 'mean_fit_time']].sort_values(by = 'mean_test_score' \\\n",
    "#                     , ascending = False) #mean = averaged over the k folds (in this case, the 3 partitions of the cross validation); std = standard deviation\n",
    "\n",
    "# scoresCV.head(10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
