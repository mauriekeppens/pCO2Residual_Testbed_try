{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d01a373b-88c7-472c-b92e-cc5120d49c7d",
   "metadata": {},
   "source": [
    "<span style=\"color:green; font-size:50px; font-weight:bold;\">Extreme Gradient Boosting - LDEO</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4647fbee-e926-411f-b0b1-4c8910a09fe9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# standard imports\n",
    "import os\n",
    "import datetime\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "import scipy\n",
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import glob\n",
    "pd.set_option('display.max_colwidth',100)\n",
    "import joblib\n",
    "import pickle\n",
    "import csv\n",
    "\n",
    "# machine learning libraries\n",
    "import sklearn            # machine-learning libary with many algorithms implemented\n",
    "from sklearn.metrics import make_scorer\n",
    "import xgboost as xgb     # extreme gradient boosting (XGB)\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Python file with supporting functions\n",
    "import residual_utils as supporting_functions\n",
    "\n",
    "import gcsfs\n",
    "fs = gcsfs.GCSFileSystem()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98950c7e-7fd1-4f2e-bbef-018c7042de90",
   "metadata": {},
   "source": [
    "<span style=\"color:hotpink; font-size:40px; font-weight:bold;\">Setting date range</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5f6e3009-8569-441b-bbd6-fe703b5297f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define date range\n",
    "date_range_start = '1982-02-01T00:00:00.000000000'\n",
    "date_range_end = '2023-12-31T00:00:00.000000000'\n",
    "\n",
    "# create date vector, adds 14 days to start & end\n",
    "dates = pd.date_range(start=date_range_start, \n",
    "                      end=date_range_end,freq='MS')\n",
    "\n",
    "init_date = str(dates[0].year) + format(dates[0].month,'02d')\n",
    "fin_date = str(dates[-1].year) + format(dates[-1].month,'02d')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f3404c8-8def-4597-b83f-e775744958ab",
   "metadata": {},
   "source": [
    "<span style=\"color:hotpink; font-size:40px; font-weight:bold;\">Setting paths and choosing grid search approach</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cf5cbc3-c215-456f-99ac-ef9748dd4892",
   "metadata": {},
   "source": [
    "<span style=\"color:lightblue; font-size:30px; font-weight:bold;\">Grid search approach</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "231fff3a-5f02-4a47-a363-b28cb21426de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for grid search, we can specify the metric to optimize for\n",
    "# the options we can currently choose between are 'nmse' (negative mean square error) and 'bias' \n",
    "\n",
    "### DEFINE APPROACH HERE: ###\n",
    "grid_search_approach = 'nmse'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f5678ff-9325-491d-a6f6-194ff04f16bf",
   "metadata": {},
   "source": [
    "<span style=\"color:lightblue; font-size:30px; font-weight:bold;\">Paths</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "75ef517f-cf44-4120-b171-cf9e19d6d7de",
   "metadata": {},
   "outputs": [],
   "source": [
    "### set paths ###\n",
    "\n",
    "your_username = \"mauriekeppens\" # leap pangeo username, for bucket. should be your github username\n",
    "\n",
    "### paths for loading: ###\n",
    "\n",
    "# where machine learning inputs are saved\n",
    "# MLinputs_path = f\"gs://leap-persistent/{your_username}\" -> change later when I have ran all dataframes\n",
    "MLinputs_path_abby = f\"gs://leap-persistent/abbysh/pco2_all_members_1982-2023/post01_xgb_inputs\"\n",
    "\n",
    "path_seeds = \"gs://leap-persistent/abbysh/pickles/random_seeds.npy\" # random seeds for ML\n",
    "\n",
    "#########################################\n",
    "\n",
    "### paths for saving: ###\n",
    "\n",
    "output_dir = f'gs://leap-persistent/{your_username}/Ensemble_Testbed/02_ML_results/XGBOOST/{grid_search_approach}' # where to save machine learning results -> in the XGBOOST\n",
    "model_output_dir = f\"{output_dir}/trained\" # where to save ML models\n",
    "recon_output_dir = f\"{output_dir}/reconstructions\" # where to save ML reconstructions\n",
    "\n",
    "metrics_output_dir = f'{output_dir}/metrics' # where to save performance metrics\n",
    "test_perform_fname = f\"{metrics_output_dir}/xgb_test_performance_{init_date}-{fin_date}.csv\" # path for test performance metrics\n",
    "unseen_perform_fname = f\"{metrics_output_dir}/xgb_unseen_performance_{init_date}-{fin_date}.csv\" # path for unseen performance metrics\n",
    "\n",
    "xgb_model_save_dir = f'{output_dir}/saved_models_{init_date}-{fin_date}' # where to save .json model file\n",
    "\n",
    "#########################################\n",
    "jobs = 30"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a6a90c3-1b00-4180-bcb2-4e3ad5caebcc",
   "metadata": {},
   "source": [
    "<span style=\"color:hotpink; font-size:40px; font-weight:bold;\">Hyperparameter selection</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "12c5c53e-b32f-44b9-b069-338778da7ba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Alternative Parameter grids, wider and more zoomed in ###\n",
    "\n",
    "# stage one for wider grid:\n",
    "# xg_param_grid = {\"n_estimators\":[50, 200, 500],\n",
    "#                  \"max_depth\":[4, 10, None],\n",
    "#                  \"\"\n",
    "#                 }\n",
    "\n",
    "# stage two zooming in\n",
    "# xg_param_grid = {\"n_estimators\":[50, 500, 1000, 4000],\n",
    "#                  \"max_depth\":[6, 7, 10, 15],\n",
    "#                  \"learning_rate\":[0.05, 0.10, 0.30]\n",
    "#                 }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cde658f7-032a-4cee-9c85-9c071b190683",
   "metadata": {},
   "outputs": [],
   "source": [
    "# these are the parameters to try for grid search. These are all tried in combination with each other\n",
    "\n",
    "xg_param_grid = {\"n_estimators\":[500, 1000, 2000, 4000],\n",
    "                 \"max_depth\":[6, 7, 10, 15],\n",
    "                 \"learning_rate\":[0.05, 0.10, 0.30, 0.40]\n",
    "                }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "983d14c8-aac6-4f06-99ec-a801348b2908",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you already know what ML parameters you want to use, define them in the dictionary \"best_params_dict\" below:\n",
    "\n",
    "# best_params_dict = {\"n_estimators\": 4000,\n",
    "#                  \"max_depth\": 6,\n",
    "#                  \"learning_rate\": 0.30\n",
    "#                 }\n",
    "\n",
    "# if you've done grid search and the results are saved in a pickle file, and you want to use the results directly from the pickle, use \"best_params_dict = None\"\n",
    "# the path is below in the XGBoost cell\n",
    "\n",
    "best_params_dict = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e83a7c8-5581-4ecf-b2a8-6e7682f961ff",
   "metadata": {},
   "source": [
    "<span style=\"color:lightblue; font-size:30px; font-weight:bold;\">To optimize for bias</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8ee43512-99bd-43dc-9964-1d9e1d90e7b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if optimizing for bias, this is the function to score for low absolute value of bias.\n",
    "# if optimizing for negative mean square error, this is not necessary \n",
    "\n",
    "def bias_fxn(truth,pred):\n",
    "    bias = pred - truth\n",
    "    return np.abs(np.nanmean(bias))\n",
    "\n",
    "bias_scorer = make_scorer(bias_fxn, greater_is_better=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78d4366f-a48d-49c7-a95c-ad424d261abe",
   "metadata": {},
   "source": [
    "<span style=\"color:hotpink; font-size:40px; font-weight:bold;\">Loading list of ESMs and members in testbed</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51eb6c2b-77c9-47cd-bac3-a05b5e3d302b",
   "metadata": {},
   "source": [
    "<span style=\"color:lightblue; font-size:30px; font-weight:bold;\">For all members for each ESM</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c6ee86a9-7d50-4b42-9e23-3250bb1e3d62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ensembles found:\n",
      "['ACCESS-ESM1-5', 'CESM2', 'CESM2-WACCM', 'CMCC-ESM2', 'CanESM5', 'CanESM5-CanOE', 'GFDL-ESM4', 'MPI-ESM1-2-LR', 'UKESM1-0-LL']\n",
      "\n",
      "Members per ensemble:\n",
      "ACCESS-ESM1-5: ['member_r4i1p1f1', 'member_r5i1p1f1'] ...\n",
      "CESM2: ['member_r10i1p1f1', 'member_r11i1p1f1', 'member_r4i1p1f1'] ...\n",
      "CESM2-WACCM: ['member_r1i1p1f1', 'member_r2i1p1f1', 'member_r3i1p1f1'] ...\n",
      "CMCC-ESM2: ['member_r1i1p1f1'] ...\n",
      "CanESM5: ['member_r10i1p2f1', 'member_r1i1p1f1', 'member_r1i1p2f1', 'member_r2i1p1f1', 'member_r2i1p2f1'] ...\n",
      "CanESM5-CanOE: ['member_r1i1p2f1', 'member_r2i1p2f1', 'member_r3i1p2f1'] ...\n",
      "GFDL-ESM4: ['member_r1i1p1f1'] ...\n",
      "MPI-ESM1-2-LR: ['member_r11i1p1f1', 'member_r12i1p1f1', 'member_r14i1p1f1', 'member_r15i1p1f1', 'member_r16i1p1f1'] ...\n",
      "UKESM1-0-LL: ['member_r1i1p1f2', 'member_r2i1p1f2', 'member_r3i1p1f2', 'member_r4i1p1f2', 'member_r8i1p1f2'] ...\n",
      "\n",
      "Seed locations:\n",
      "ACCESS-ESM1-5: {'member_r4i1p1f1': 0, 'member_r5i1p1f1': 1} ...\n",
      "CESM2: {'member_r10i1p1f1': 0, 'member_r11i1p1f1': 1, 'member_r4i1p1f1': 2} ...\n",
      "CESM2-WACCM: {'member_r1i1p1f1': 0, 'member_r2i1p1f1': 1, 'member_r3i1p1f1': 2} ...\n",
      "CMCC-ESM2: {'member_r1i1p1f1': 0} ...\n",
      "CanESM5: {'member_r10i1p2f1': 0, 'member_r1i1p1f1': 1, 'member_r1i1p2f1': 2, 'member_r2i1p1f1': 3, 'member_r2i1p2f1': 4} ...\n",
      "CanESM5-CanOE: {'member_r1i1p2f1': 0, 'member_r2i1p2f1': 1, 'member_r3i1p2f1': 2} ...\n",
      "GFDL-ESM4: {'member_r1i1p1f1': 0} ...\n",
      "MPI-ESM1-2-LR: {'member_r11i1p1f1': 0, 'member_r12i1p1f1': 1, 'member_r14i1p1f1': 2, 'member_r15i1p1f1': 3, 'member_r16i1p1f1': 4} ...\n",
      "UKESM1-0-LL: {'member_r1i1p1f2': 0, 'member_r2i1p1f2': 1, 'member_r3i1p1f2': 2, 'member_r4i1p1f2': 3, 'member_r8i1p1f2': 4} ...\n"
     ]
    }
   ],
   "source": [
    "### loads list of Earth System Models (\"ensembles\") and members for the full testbed ###\n",
    "\n",
    "# building a list of ensembles\n",
    "ensembles = []\n",
    "for path in fs.ls(MLinputs_path_abby):\n",
    "    ens = path.split('/')[-1].split('.')[0]\n",
    "    if ens not in ensembles:\n",
    "        ensembles.append(ens)\n",
    "\n",
    "# Build a dictionary of members for each ensemble\n",
    "mems_dict = dict()\n",
    "a = fs.ls(MLinputs_path_abby)\n",
    "for ens_path in a:\n",
    "    ens = ens_path.split('/')[-1]\n",
    "    mems = fs.ls(ens_path)\n",
    "    for mem in mems:\n",
    "        memo = mem.split('/')[-1]\n",
    "        \n",
    "        if ens not in mems_dict:\n",
    "            mems_dict[ens] = [memo]\n",
    "\n",
    "        elif ens in mems_dict:\n",
    "            mems_dict[ens].append(memo)\n",
    "\n",
    "# loads random seeds for ML\n",
    "random_seeds = np.load(fs.open(path_seeds))   \n",
    "\n",
    "# Build a seed location dictionary\n",
    "seed_loc_dict = defaultdict(dict)\n",
    "for ens,mem_list in mems_dict.items():\n",
    "    sub_dictt = dict()\n",
    "    for no,mem in enumerate(mem_list):\n",
    "        sub_dictt.update({mem:no})\n",
    "    seed_loc_dict.update({ens:sub_dictt})\n",
    "\n",
    "# Print results\n",
    "print(\"Ensembles found:\")\n",
    "print(ensembles)\n",
    "\n",
    "print(\"\\nMembers per ensemble:\")\n",
    "for ens, mem_list in mems_dict.items():\n",
    "    print(f\"{ens}: {mem_list[:5]} ...\")  # show only first 5 members if there are many\n",
    "\n",
    "print(\"\\nSeed locations:\")\n",
    "for ens, seed_dict in seed_loc_dict.items():\n",
    "    print(f\"{ens}: {dict(list(seed_dict.items())[:5])} ...\")  # first 5 memberâ†’index\n",
    "    \n",
    "# we have in random_seeds an array of numbers for reproducibility -> e.g. 42, 1337, 2025,...\n",
    "# each ensemble has multiple members. -> seed_loc_dict maps each member to an integer index\n",
    "# So each member gets a unique random seed, ensuring consistent reproducibility across runs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95e0fafc-a8ea-4689-aa0c-a48f0bd6d781",
   "metadata": {},
   "source": [
    "<span style=\"color:lightblue; font-size:30px; font-weight:bold;\">For one member per ESM</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0d9a441a-e38f-491d-b4aa-1e4f175097e0",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'old_ensemble_dir' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[32], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m### if you only want one member per model ###\u001b[39;00m\n\u001b[1;32m      3\u001b[0m ensembles \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m path \u001b[38;5;129;01min\u001b[39;00m fs\u001b[38;5;241m.\u001b[39mls(\u001b[43mold_ensemble_dir\u001b[49m):\n\u001b[1;32m      5\u001b[0m     ens \u001b[38;5;241m=\u001b[39m path\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ens \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ensembles:\n",
      "\u001b[0;31mNameError\u001b[0m: name 'old_ensemble_dir' is not defined"
     ]
    }
   ],
   "source": [
    "### if you only want one member per model ###\n",
    "\n",
    "ensembles = []\n",
    "for path in fs.ls(old_ensemble_dir):\n",
    "    ens = path.split('/')[-1].split('.')[0]\n",
    "    if ens not in ensembles:\n",
    "        ensembles.append(ens)\n",
    "\n",
    "mems_dict = dict()\n",
    "a = fs.ls(old_ensemble_dir)\n",
    "for ens_path in a:\n",
    "    ens = ens_path.split('/')[-1]\n",
    "    mems = fs.ls(ens_path)\n",
    "    \n",
    "    ### difference is here, only add one member per ensemble to dictionary\n",
    "    for mem in mems[0:1]:\n",
    "        memo = mem.split('/')[-1]\n",
    "        \n",
    "        if ens not in mems_dict:\n",
    "            mems_dict[ens] = [memo]\n",
    "\n",
    "        elif ens in mems_dict:\n",
    "            mems_dict[ens].append(memo)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be932ec2-3c74-403d-9056-269a3032511e",
   "metadata": {},
   "source": [
    "<span style=\"color:hotpink; font-size:40px; font-weight:bold;\">Specifiying feature and target variables</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d0746bd4-0cbe-438b-bea0-dbee75430652",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Feature and target variables for use in the ML model ###\n",
    "\n",
    "# features for ML:\n",
    "features_sel = ['sst','sst_anom','sss','sss_anom','mld_clim_log','chl_log','chl_log_anom','xco2','A', 'B', 'C', 'T0', 'T1']\n",
    "\n",
    "# the target variable we reconstruct:\n",
    "target_sel = ['pco2_residual'] # this represents pCO2 - pCO2_T (calculated in notebook 00)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1d9286f-5247-4764-8b57-76e0fd5d6c21",
   "metadata": {},
   "source": [
    "<span style=\"color:hotpink; font-size:40px; font-weight:bold;\">Isolating training, testing, and validation sets</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "806dd9fc-98c4-4d38-8d8c-cbc5246779b1",
   "metadata": {},
   "source": [
    "We want to make sure we don't use the same data for training as we do for testing.\n",
    "We also want to make sure that we don't use the same months from every year for either testing or training. \n",
    "For example, we don't want the month of May from each year to always be used for testing.\n",
    "\n",
    "Therefore, the way we isolate the training and test sets is as follows:\n",
    "Let's say we are using a testbed from January 1982 through December 2023. \n",
    "This is 504 months in total. Below, the code is counting 0 through 504.\n",
    "If the number that the counting loop is at is divisible by 5, the associated month is used as a testing month. Otherwise, that month is used for training.\n",
    "As a result, for each year in the testbed, two or three months will be used for testing and the rest for training. But, the specific months vary per year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4efeb1e6-3133-4357-8558-500ed254063f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training/validation dates: 402\n",
      "First 10 training/validation dates: ['1982-3', '1982-4', '1982-5', '1982-6', '1982-8', '1982-9', '1982-10', '1982-11', '1983-1', '1983-2']\n",
      "\n",
      "Number of test dates: 101\n",
      "First 10 test dates: ['1982-2', '1982-7', '1982-12', '1983-5', '1983-10', '1984-3', '1984-8', '1985-1', '1985-6', '1985-11']\n"
     ]
    }
   ],
   "source": [
    "### train-validate-test split proportions ###\n",
    "val_prop = .2 # 20% of data for validation\n",
    "test_prop = .2 # 20% of data for testing\n",
    "\n",
    "select_dates = []\n",
    "test_dates = []\n",
    "\n",
    "for i in range(0,len(dates)):\n",
    "    \n",
    "    if i % 5 != 0:\n",
    "        select_dates.append(dates[i]) ### train days set ###\n",
    "    if i % 5 == 0:\n",
    "        test_dates.append(dates[i]) ### test days set ### \n",
    "\n",
    "### Then, the month numbers above are converted back to their respective datetime objects.\n",
    "\n",
    "year_mon = []\n",
    "\n",
    "for i in range(0,len(select_dates)):\n",
    "    \n",
    "    tmp = select_dates[i]\n",
    "    year_mon.append(f\"{tmp.year}-{tmp.month}\")\n",
    "    \n",
    "test_year_mon = []\n",
    "\n",
    "for i in range(0,len(test_dates)):\n",
    "    \n",
    "    tmp = test_dates[i]\n",
    "    test_year_mon.append(f\"{tmp.year}-{tmp.month}\")\n",
    "\n",
    "# Print summaries\n",
    "print(\"Number of training/validation dates:\", len(year_mon))\n",
    "print(\"First 10 training/validation dates:\", year_mon[:10])\n",
    "\n",
    "print(\"\\nNumber of test dates:\", len(test_year_mon))\n",
    "print(\"First 10 test dates:\", test_year_mon[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd7d8a61-89e9-4cb8-84d2-552cd9f7b307",
   "metadata": {},
   "source": [
    "<span style=\"color:hotpink; font-size:40px; font-weight:bold;\">Machine learning happens here</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fc9a2539-d618-4ead-8788-ca52b05fe164",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-09-18 17:46:58.546909\n",
      "ACCESS-ESM1-5 member_r4i1p1f1\n",
      "\n",
      "Ensemble: ACCESS-ESM1-5, Member: member_r4i1p1f1\n",
      "DataFrame shape: (32594400, 30)\n",
      "Columns: Index(['key_0', 'sss', 'sst', 'mld', 'chl', 'pco2_residual', 'spco2',\n",
      "       'socat_mask', 'chl_sat', 'mld_log', 'mld_anom', 'mld_log_anom',\n",
      "       'mld_clim', 'mld_clim_log', 'chl_log', 'chl_log_anom', 'chl_sat_log',\n",
      "       'chl_sat_anom', 'sss_anom', 'sst_anom', 'T0', 'T1', 'A', 'B', 'C',\n",
      "       'net_mask', 'xco2', 'year', 'mon', 'year_month'],\n",
      "      dtype='object')\n",
      "First rows:\n",
      "                          key_0  sss  sst  mld  chl  pco2_residual  spco2  \\\n",
      "time       xlon   ylat                                                     \n",
      "1982-02-01 -179.5 -89.5      2  NaN  NaN  NaN  NaN            NaN    NaN   \n",
      "                  -88.5      2  NaN  NaN  NaN  NaN            NaN    NaN   \n",
      "                  -87.5      2  NaN  NaN  NaN  NaN            NaN    NaN   \n",
      "                  -86.5      2  NaN  NaN  NaN  NaN            NaN    NaN   \n",
      "                  -85.5      2  NaN  NaN  NaN  NaN            NaN    NaN   \n",
      "\n",
      "                         socat_mask  chl_sat  mld_log  ...        T0  \\\n",
      "time       xlon   ylat                                 ...             \n",
      "1982-02-01 -179.5 -89.5         0.0      NaN      NaN  ...  0.852078   \n",
      "                  -88.5         0.0      NaN      NaN  ...  0.852078   \n",
      "                  -87.5         0.0      NaN      NaN  ...  0.852078   \n",
      "                  -86.5         0.0      NaN      NaN  ...  0.852078   \n",
      "                  -85.5         0.0      NaN      NaN  ...  0.852078   \n",
      "\n",
      "                               T1         A         B         C  net_mask  \\\n",
      "time       xlon   ylat                                                      \n",
      "1982-02-01 -179.5 -89.5  0.523416 -0.999962 -0.000076  0.008726       NaN   \n",
      "                  -88.5  0.523416 -0.999657 -0.000228  0.026176       NaN   \n",
      "                  -87.5  0.523416 -0.999048 -0.000381  0.043618       NaN   \n",
      "                  -86.5  0.523416 -0.998135 -0.000533  0.061046       NaN   \n",
      "                  -85.5  0.523416 -0.996917 -0.000685  0.078456       NaN   \n",
      "\n",
      "                               xco2  year  mon  year_month  \n",
      "time       xlon   ylat                                      \n",
      "1982-02-01 -179.5 -89.5  341.661926  1982    2      1982-2  \n",
      "                  -88.5  341.661926  1982    2      1982-2  \n",
      "                  -87.5  341.661926  1982    2      1982-2  \n",
      "                  -86.5  341.661926  1982    2      1982-2  \n",
      "                  -85.5  341.661926  1982    2      1982-2  \n",
      "\n",
      "[5 rows x 30 columns]\n",
      "Total rows after filter the data: 16939974\n",
      "Number of filtered points corresponding to SOCAT: 297573\n",
      "Number of training points: 238718\n",
      "Number of testing points: 58855\n",
      "Number of unseen points: 16642401\n",
      "Xtrain shape: (238718, 13)\n",
      "ytrain shape: (238718,)\n",
      "X_test shape: (58855, 13)\n",
      "y_test shape: (58855,)\n",
      "gs://leap-persistent/mauriekeppens/Ensemble_Testbed/02_ML_results/XGBOOST/nmse/metrics/nmse_best_params_dict_198202-202312_ACCESS-ESM1-5.pickle\n",
      "Best hyperparameters: {'learning_rate': 0.05, 'max_depth': 10, 'n_estimators': 4000}\n",
      "y_test length: 58855\n",
      "y_pred_test length: 58855\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/srv/conda/envs/notebook/lib/python3.12/site-packages/gcsfs/core.py:1865: UserWarning: Append mode 'a' is not supported in GCS. Using overwrite mode instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test performance metrics: {'mse': 69.45649136902385, 'mae': 5.293047913915207, 'medae': 3.4573132106062303, 'max_error': 172.88574063996077, 'bias': 0.16168219666709582, 'r2': 0.9293837437086994, 'corr': 0.9647763243382423, 'cent_rmse': 8.332487629497209, 'stdev': 29.09101, 'amp_ratio': 0.7955313918087857, 'stdev_ref': 31.362023475346902, 'range_ref': 445.85411982281875, 'iqr_ref': 35.39009281133522}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/srv/conda/envs/notebook/lib/python3.12/site-packages/gcsfs/core.py:1865: UserWarning: Append mode 'a' is not supported in GCS. Using overwrite mode instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unseen performance metrics: {'mse': 120.11119129551156, 'mae': 6.917735108534175, 'medae': 4.585168425708957, 'max_error': 265.2407128060157, 'bias': 0.9918781548554838, 'r2': 0.8751175797253312, 'corr': 0.9369337075504994, 'cent_rmse': 10.914548443128798, 'stdev': 27.776644, 'amp_ratio': 0.8888965109032655, 'stdev_ref': 31.012807521660378, 'range_ref': 499.6785690718387, 'iqr_ref': 38.86167681217614}\n",
      "Starting reconstruction saving process\n",
      "gs://leap-persistent/mauriekeppens/Ensemble_Testbed/02_ML_results/XGBOOST/nmse/reconstructions/ACCESS-ESM1-5/member_r4i1p1f1/recon_pCO2residual_ACCESS-ESM1-5_member_r4i1p1f1_mon_1x1_198202_202312.zarr\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/srv/conda/envs/notebook/lib/python3.12/site-packages/zarr/api/asynchronous.py:227: UserWarning: Consolidated metadata is currently not part in the Zarr format 3 specification. It may not be supported by other zarr implementations and may change in the future.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Save complete\n",
      "end of member 2025-09-18 18:02:34.783042\n",
      "ACCESS-ESM1-5 member_r5i1p1f1\n",
      "\n",
      "Ensemble: ACCESS-ESM1-5, Member: member_r5i1p1f1\n",
      "DataFrame shape: (32594400, 30)\n",
      "Columns: Index(['key_0', 'sss', 'sst', 'mld', 'chl', 'pco2_residual', 'spco2',\n",
      "       'socat_mask', 'chl_sat', 'mld_log', 'mld_anom', 'mld_log_anom',\n",
      "       'mld_clim', 'mld_clim_log', 'chl_log', 'chl_log_anom', 'chl_sat_log',\n",
      "       'chl_sat_anom', 'sss_anom', 'sst_anom', 'T0', 'T1', 'A', 'B', 'C',\n",
      "       'net_mask', 'xco2', 'year', 'mon', 'year_month'],\n",
      "      dtype='object')\n",
      "First rows:\n",
      "                          key_0  sss  sst  mld  chl  pco2_residual  spco2  \\\n",
      "time       xlon   ylat                                                     \n",
      "1982-02-01 -179.5 -89.5      2  NaN  NaN  NaN  NaN            NaN    NaN   \n",
      "                  -88.5      2  NaN  NaN  NaN  NaN            NaN    NaN   \n",
      "                  -87.5      2  NaN  NaN  NaN  NaN            NaN    NaN   \n",
      "                  -86.5      2  NaN  NaN  NaN  NaN            NaN    NaN   \n",
      "                  -85.5      2  NaN  NaN  NaN  NaN            NaN    NaN   \n",
      "\n",
      "                         socat_mask  chl_sat  mld_log  ...        T0  \\\n",
      "time       xlon   ylat                                 ...             \n",
      "1982-02-01 -179.5 -89.5         0.0      NaN      NaN  ...  0.852078   \n",
      "                  -88.5         0.0      NaN      NaN  ...  0.852078   \n",
      "                  -87.5         0.0      NaN      NaN  ...  0.852078   \n",
      "                  -86.5         0.0      NaN      NaN  ...  0.852078   \n",
      "                  -85.5         0.0      NaN      NaN  ...  0.852078   \n",
      "\n",
      "                               T1         A         B         C  net_mask  \\\n",
      "time       xlon   ylat                                                      \n",
      "1982-02-01 -179.5 -89.5  0.523416 -0.999962 -0.000076  0.008726       NaN   \n",
      "                  -88.5  0.523416 -0.999657 -0.000228  0.026176       NaN   \n",
      "                  -87.5  0.523416 -0.999048 -0.000381  0.043618       NaN   \n",
      "                  -86.5  0.523416 -0.998135 -0.000533  0.061046       NaN   \n",
      "                  -85.5  0.523416 -0.996917 -0.000685  0.078456       NaN   \n",
      "\n",
      "                               xco2  year  mon  year_month  \n",
      "time       xlon   ylat                                      \n",
      "1982-02-01 -179.5 -89.5  341.661926  1982    2      1982-2  \n",
      "                  -88.5  341.661926  1982    2      1982-2  \n",
      "                  -87.5  341.661926  1982    2      1982-2  \n",
      "                  -86.5  341.661926  1982    2      1982-2  \n",
      "                  -85.5  341.661926  1982    2      1982-2  \n",
      "\n",
      "[5 rows x 30 columns]\n",
      "Total rows after filter the data: 16941401\n",
      "Number of filtered points corresponding to SOCAT: 297625\n",
      "Number of training points: 238758\n",
      "Number of testing points: 58867\n",
      "Number of unseen points: 16643776\n",
      "Xtrain shape: (238758, 13)\n",
      "ytrain shape: (238758,)\n",
      "X_test shape: (58867, 13)\n",
      "y_test shape: (58867,)\n",
      "gs://leap-persistent/mauriekeppens/Ensemble_Testbed/02_ML_results/XGBOOST/nmse/metrics/nmse_best_params_dict_198202-202312_ACCESS-ESM1-5.pickle\n",
      "Best hyperparameters: {'learning_rate': 0.05, 'max_depth': 10, 'n_estimators': 4000}\n",
      "y_test length: 58867\n",
      "y_pred_test length: 58867\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/srv/conda/envs/notebook/lib/python3.12/site-packages/gcsfs/core.py:1865: UserWarning: Append mode 'a' is not supported in GCS. Using overwrite mode instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test performance metrics: {'mse': 59.74242359647672, 'mae': 4.932140104911187, 'medae': 3.315842096489291, 'max_error': 182.4172593931006, 'bias': 0.09310281024101652, 'r2': 0.9318922585267697, 'corr': 0.9657649510691455, 'cent_rmse': 7.7287615661806885, 'stdev': 27.765606, 'amp_ratio': 0.6034514518107523, 'stdev_ref': 29.617144142129014, 'range_ref': 445.0125295505422, 'iqr_ref': 34.875438800250265}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/srv/conda/envs/notebook/lib/python3.12/site-packages/gcsfs/core.py:1865: UserWarning: Append mode 'a' is not supported in GCS. Using overwrite mode instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unseen performance metrics: {'mse': 105.13476877539297, 'mae': 6.525238788644932, 'medae': 4.426642657462764, 'max_error': 254.12456587082517, 'bias': 0.6661003369186755, 'r2': 0.8834755951741112, 'corr': 0.9412330745344419, 'cent_rmse': 10.231866208180255, 'stdev': 26.94513, 'amp_ratio': 0.8687254195264051, 'stdev_ref': 30.037565806412346, 'range_ref': 499.56449442516737, 'iqr_ref': 38.21944091749795}\n",
      "Starting reconstruction saving process\n",
      "gs://leap-persistent/mauriekeppens/Ensemble_Testbed/02_ML_results/XGBOOST/nmse/reconstructions/ACCESS-ESM1-5/member_r5i1p1f1/recon_pCO2residual_ACCESS-ESM1-5_member_r5i1p1f1_mon_1x1_198202_202312.zarr\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/srv/conda/envs/notebook/lib/python3.12/site-packages/zarr/api/asynchronous.py:227: UserWarning: Consolidated metadata is currently not part in the Zarr format 3 specification. It may not be supported by other zarr implementations and may change in the future.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Save complete\n",
      "end of member 2025-09-18 18:18:10.278098\n",
      "CESM2 member_r10i1p1f1\n",
      "\n",
      "Ensemble: CESM2, Member: member_r10i1p1f1\n",
      "DataFrame shape: (32594400, 30)\n",
      "Columns: Index(['key_0', 'sss', 'sst', 'mld', 'chl', 'pco2_residual', 'spco2',\n",
      "       'socat_mask', 'chl_sat', 'mld_log', 'mld_anom', 'mld_log_anom',\n",
      "       'mld_clim', 'mld_clim_log', 'chl_log', 'chl_log_anom', 'chl_sat_log',\n",
      "       'chl_sat_anom', 'sss_anom', 'sst_anom', 'T0', 'T1', 'A', 'B', 'C',\n",
      "       'net_mask', 'xco2', 'year', 'mon', 'year_month'],\n",
      "      dtype='object')\n",
      "First rows:\n",
      "                          key_0  sss  sst  mld  chl  pco2_residual  spco2  \\\n",
      "time       xlon   ylat                                                     \n",
      "1982-02-01 -179.5 -89.5      2  NaN  NaN  NaN  NaN            NaN    NaN   \n",
      "                  -88.5      2  NaN  NaN  NaN  NaN            NaN    NaN   \n",
      "                  -87.5      2  NaN  NaN  NaN  NaN            NaN    NaN   \n",
      "                  -86.5      2  NaN  NaN  NaN  NaN            NaN    NaN   \n",
      "                  -85.5      2  NaN  NaN  NaN  NaN            NaN    NaN   \n",
      "\n",
      "                         socat_mask  chl_sat  mld_log  ...        T0  \\\n",
      "time       xlon   ylat                                 ...             \n",
      "1982-02-01 -179.5 -89.5         0.0      NaN      NaN  ...  0.852078   \n",
      "                  -88.5         0.0      NaN      NaN  ...  0.852078   \n",
      "                  -87.5         0.0      NaN      NaN  ...  0.852078   \n",
      "                  -86.5         0.0      NaN      NaN  ...  0.852078   \n",
      "                  -85.5         0.0      NaN      NaN  ...  0.852078   \n",
      "\n",
      "                               T1         A         B         C  net_mask  \\\n",
      "time       xlon   ylat                                                      \n",
      "1982-02-01 -179.5 -89.5  0.523416 -0.999962 -0.000076  0.008726       NaN   \n",
      "                  -88.5  0.523416 -0.999657 -0.000228  0.026176       NaN   \n",
      "                  -87.5  0.523416 -0.999048 -0.000381  0.043618       NaN   \n",
      "                  -86.5  0.523416 -0.998135 -0.000533  0.061046       NaN   \n",
      "                  -85.5  0.523416 -0.996917 -0.000685  0.078456       NaN   \n",
      "\n",
      "                               xco2  year  mon  year_month  \n",
      "time       xlon   ylat                                      \n",
      "1982-02-01 -179.5 -89.5  341.661926  1982    2      1982-2  \n",
      "                  -88.5  341.661926  1982    2      1982-2  \n",
      "                  -87.5  341.661926  1982    2      1982-2  \n",
      "                  -86.5  341.661926  1982    2      1982-2  \n",
      "                  -85.5  341.661926  1982    2      1982-2  \n",
      "\n",
      "[5 rows x 30 columns]\n",
      "Total rows after filter the data: 16966190\n",
      "Number of filtered points corresponding to SOCAT: 300566\n",
      "Number of training points: 241122\n",
      "Number of testing points: 59444\n",
      "Number of unseen points: 16665624\n",
      "Xtrain shape: (241122, 13)\n",
      "ytrain shape: (241122,)\n",
      "X_test shape: (59444, 13)\n",
      "y_test shape: (59444,)\n",
      "gs://leap-persistent/mauriekeppens/Ensemble_Testbed/02_ML_results/XGBOOST/nmse/metrics/nmse_best_params_dict_198202-202312_CESM2.pickle\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "b/leap-persistent/o/mauriekeppens%2FEnsemble_Testbed%2F02_ML_results%2FXGBOOST%2Fnmse%2Fmetrics%2Fnmse_best_params_dict_198202-202312_CESM2.pickle",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 161\u001b[0m\n\u001b[1;32m    159\u001b[0m     best_params_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmetrics_output_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mgrid_search_approach\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_best_params_dict_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00minit_date\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m-\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfin_date\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mens\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.pickle\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    160\u001b[0m     \u001b[38;5;28mprint\u001b[39m(best_params_path)\n\u001b[0;32m--> 161\u001b[0m     best_params \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_pickle\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbest_params_path\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto_dict(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrecords\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    162\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBest hyperparameters:\u001b[39m\u001b[38;5;124m\"\u001b[39m, best_params)\n\u001b[1;32m    164\u001b[0m \u001b[38;5;66;03m### Running ML using hyperparameters: setting up ML model and then training it (\".fit\") on training set\u001b[39;00m\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.12/site-packages/pandas/io/pickle.py:185\u001b[0m, in \u001b[0;36mread_pickle\u001b[0;34m(filepath_or_buffer, compression, storage_options)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;124;03mLoad pickled pandas object (or any object) from file.\u001b[39;00m\n\u001b[1;32m    125\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    182\u001b[0m \u001b[38;5;124;03m4    4    9\u001b[39;00m\n\u001b[1;32m    183\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    184\u001b[0m excs_to_catch \u001b[38;5;241m=\u001b[39m (\u001b[38;5;167;01mAttributeError\u001b[39;00m, \u001b[38;5;167;01mImportError\u001b[39;00m, \u001b[38;5;167;01mModuleNotFoundError\u001b[39;00m, \u001b[38;5;167;01mTypeError\u001b[39;00m)\n\u001b[0;32m--> 185\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    186\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    187\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    188\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    189\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    190\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    191\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m handles:\n\u001b[1;32m    192\u001b[0m     \u001b[38;5;66;03m# 1) try standard library Pickle\u001b[39;00m\n\u001b[1;32m    193\u001b[0m     \u001b[38;5;66;03m# 2) try pickle_compat (older pandas version) to handle subclass changes\u001b[39;00m\n\u001b[1;32m    194\u001b[0m     \u001b[38;5;66;03m# 3) try pickle_compat with latin-1 encoding upon a UnicodeDecodeError\u001b[39;00m\n\u001b[1;32m    196\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    197\u001b[0m         \u001b[38;5;66;03m# TypeError for Cython complaints about object.__new__ vs Tick.__new__\u001b[39;00m\n\u001b[1;32m    198\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.12/site-packages/pandas/io/common.py:728\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    725\u001b[0m     codecs\u001b[38;5;241m.\u001b[39mlookup_error(errors)\n\u001b[1;32m    727\u001b[0m \u001b[38;5;66;03m# open URLs\u001b[39;00m\n\u001b[0;32m--> 728\u001b[0m ioargs \u001b[38;5;241m=\u001b[39m \u001b[43m_get_filepath_or_buffer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    729\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath_or_buf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    730\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    731\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    732\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    733\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    734\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    736\u001b[0m handle \u001b[38;5;241m=\u001b[39m ioargs\u001b[38;5;241m.\u001b[39mfilepath_or_buffer\n\u001b[1;32m    737\u001b[0m handles: \u001b[38;5;28mlist\u001b[39m[BaseBuffer]\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.12/site-packages/pandas/io/common.py:432\u001b[0m, in \u001b[0;36m_get_filepath_or_buffer\u001b[0;34m(filepath_or_buffer, encoding, compression, mode, storage_options)\u001b[0m\n\u001b[1;32m    427\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m    429\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    430\u001b[0m     file_obj \u001b[38;5;241m=\u001b[39m \u001b[43mfsspec\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    431\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfsspec_mode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m--> 432\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    433\u001b[0m \u001b[38;5;66;03m# GH 34626 Reads from Public Buckets without Credentials needs anon=True\u001b[39;00m\n\u001b[1;32m    434\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;28mtuple\u001b[39m(err_types_to_retry_with_anon):\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.12/site-packages/fsspec/core.py:147\u001b[0m, in \u001b[0;36mOpenFile.open\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mopen\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    141\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Materialise this as a real open file without context\u001b[39;00m\n\u001b[1;32m    142\u001b[0m \n\u001b[1;32m    143\u001b[0m \u001b[38;5;124;03m    The OpenFile object should be explicitly closed to avoid enclosed file\u001b[39;00m\n\u001b[1;32m    144\u001b[0m \u001b[38;5;124;03m    instances persisting. You must, therefore, keep a reference to the OpenFile\u001b[39;00m\n\u001b[1;32m    145\u001b[0m \u001b[38;5;124;03m    during the life of the file-like it generates.\u001b[39;00m\n\u001b[1;32m    146\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 147\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__enter__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.12/site-packages/fsspec/core.py:105\u001b[0m, in \u001b[0;36mOpenFile.__enter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    102\u001b[0m mode \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mt\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 105\u001b[0m     f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    106\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    107\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m has_magic(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpath):\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.12/site-packages/fsspec/spec.py:1338\u001b[0m, in \u001b[0;36mAbstractFileSystem.open\u001b[0;34m(self, path, mode, block_size, cache_options, compression, **kwargs)\u001b[0m\n\u001b[1;32m   1336\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1337\u001b[0m     ac \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mautocommit\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_intrans)\n\u001b[0;32m-> 1338\u001b[0m     f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_open\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1339\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1340\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1341\u001b[0m \u001b[43m        \u001b[49m\u001b[43mblock_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mblock_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1342\u001b[0m \u001b[43m        \u001b[49m\u001b[43mautocommit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mac\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1343\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1344\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1345\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1346\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m compression \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1347\u001b[0m         \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mfsspec\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompression\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m compr\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.12/site-packages/gcsfs/core.py:1676\u001b[0m, in \u001b[0;36mGCSFileSystem._open\u001b[0;34m(self, path, mode, block_size, cache_options, acl, consistency, metadata, autocommit, fixed_key_metadata, generation, **kwargs)\u001b[0m\n\u001b[1;32m   1674\u001b[0m     block_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefault_block_size\n\u001b[1;32m   1675\u001b[0m const \u001b[38;5;241m=\u001b[39m consistency \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconsistency\n\u001b[0;32m-> 1676\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mGCSFile\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1677\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1678\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1679\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1680\u001b[0m \u001b[43m    \u001b[49m\u001b[43mblock_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1681\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1682\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconsistency\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconst\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1683\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1684\u001b[0m \u001b[43m    \u001b[49m\u001b[43macl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43macl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1685\u001b[0m \u001b[43m    \u001b[49m\u001b[43mautocommit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mautocommit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1686\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfixed_key_metadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfixed_key_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1687\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1688\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.12/site-packages/gcsfs/core.py:1848\u001b[0m, in \u001b[0;36mGCSFile.__init__\u001b[0;34m(self, gcsfs, path, mode, block_size, autocommit, cache_type, cache_options, acl, consistency, metadata, content_type, timeout, fixed_key_metadata, generation, kms_key_name, **kwargs)\u001b[0m\n\u001b[1;32m   1846\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAttempt to open a bucket\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1847\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgeneration \u001b[38;5;241m=\u001b[39m _coalesce_generation(generation, path_generation)\n\u001b[0;32m-> 1848\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1849\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgcsfs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1850\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1851\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1852\u001b[0m \u001b[43m    \u001b[49m\u001b[43mblock_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1853\u001b[0m \u001b[43m    \u001b[49m\u001b[43mautocommit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mautocommit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1854\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1855\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1856\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1857\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1858\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgcsfs \u001b[38;5;241m=\u001b[39m gcsfs\n\u001b[1;32m   1859\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbucket \u001b[38;5;241m=\u001b[39m bucket\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.12/site-packages/fsspec/spec.py:1912\u001b[0m, in \u001b[0;36mAbstractBufferedFile.__init__\u001b[0;34m(self, fs, path, mode, block_size, autocommit, cache_type, cache_options, size, **kwargs)\u001b[0m\n\u001b[1;32m   1910\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msize \u001b[38;5;241m=\u001b[39m size\n\u001b[1;32m   1911\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1912\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msize \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdetails\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msize\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1913\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache \u001b[38;5;241m=\u001b[39m caches[cache_type](\n\u001b[1;32m   1914\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocksize, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fetch_range, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msize, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcache_options\n\u001b[1;32m   1915\u001b[0m     )\n\u001b[1;32m   1916\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.12/site-packages/gcsfs/core.py:1892\u001b[0m, in \u001b[0;36mGCSFile.details\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1889\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[1;32m   1890\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdetails\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m   1891\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_details \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1892\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_details \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minfo\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgeneration\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgeneration\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1893\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_details\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.12/site-packages/fsspec/asyn.py:118\u001b[0m, in \u001b[0;36msync_wrapper.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    117\u001b[0m     \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m obj \u001b[38;5;129;01mor\u001b[39;00m args[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m--> 118\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msync\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.12/site-packages/fsspec/asyn.py:103\u001b[0m, in \u001b[0;36msync\u001b[0;34m(loop, func, timeout, *args, **kwargs)\u001b[0m\n\u001b[1;32m    101\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m FSTimeoutError \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mreturn_result\u001b[39;00m\n\u001b[1;32m    102\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(return_result, \u001b[38;5;167;01mBaseException\u001b[39;00m):\n\u001b[0;32m--> 103\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m return_result\n\u001b[1;32m    104\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    105\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m return_result\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.12/site-packages/fsspec/asyn.py:56\u001b[0m, in \u001b[0;36m_runner\u001b[0;34m(event, coro, result, timeout)\u001b[0m\n\u001b[1;32m     54\u001b[0m     coro \u001b[38;5;241m=\u001b[39m asyncio\u001b[38;5;241m.\u001b[39mwait_for(coro, timeout\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 56\u001b[0m     result[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m coro\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m ex:\n\u001b[1;32m     58\u001b[0m     result[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m=\u001b[39m ex\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.12/site-packages/gcsfs/core.py:1036\u001b[0m, in \u001b[0;36mGCSFileSystem._info\u001b[0;34m(self, path, generation, **kwargs)\u001b[0m\n\u001b[1;32m   1034\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m:\n\u001b[1;32m   1035\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m-> 1036\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_list_objects(path, max_results\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m   1037\u001b[0m exact \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m((o \u001b[38;5;28;01mfor\u001b[39;00m o \u001b[38;5;129;01min\u001b[39;00m out \u001b[38;5;28;01mif\u001b[39;00m o[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mrstrip(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m==\u001b[39m path), \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m   1038\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m exact \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _is_directory_marker(exact):\n\u001b[1;32m   1039\u001b[0m     \u001b[38;5;66;03m# exact hit\u001b[39;00m\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.12/site-packages/gcsfs/core.py:625\u001b[0m, in \u001b[0;36mGCSFileSystem._list_objects\u001b[0;34m(self, path, prefix, versions, **kwargs)\u001b[0m\n\u001b[1;32m    623\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (items \u001b[38;5;241m+\u001b[39m pseudodirs):\n\u001b[1;32m    624\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m key:\n\u001b[0;32m--> 625\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_object(path)]\n\u001b[1;32m    626\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    627\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m []\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.12/site-packages/gcsfs/core.py:561\u001b[0m, in \u001b[0;36mGCSFileSystem._get_object\u001b[0;34m(self, path)\u001b[0m\n\u001b[1;32m    558\u001b[0m \u001b[38;5;66;03m# Work around various permission settings. Prefer an object get (storage.objects.get), but\u001b[39;00m\n\u001b[1;32m    559\u001b[0m \u001b[38;5;66;03m# fall back to a bucket list + filter to object name (storage.objects.list).\u001b[39;00m\n\u001b[1;32m    560\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 561\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(\n\u001b[1;32m    562\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGET\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb/\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m/o/\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, bucket, key, json_out\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, generation\u001b[38;5;241m=\u001b[39mgeneration\n\u001b[1;32m    563\u001b[0m     )\n\u001b[1;32m    564\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    565\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(e)\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mForbidden\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.12/site-packages/gcsfs/core.py:481\u001b[0m, in \u001b[0;36mGCSFileSystem._call\u001b[0;34m(self, method, path, json_out, info_out, *args, **kwargs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_call\u001b[39m(\n\u001b[1;32m    478\u001b[0m     \u001b[38;5;28mself\u001b[39m, method, path, \u001b[38;5;241m*\u001b[39margs, json_out\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, info_out\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    479\u001b[0m ):\n\u001b[1;32m    480\u001b[0m     logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmethod\u001b[38;5;241m.\u001b[39mupper()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00margs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mheaders\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 481\u001b[0m     status, headers, info, contents \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request(\n\u001b[1;32m    482\u001b[0m         method, path, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    483\u001b[0m     )\n\u001b[1;32m    484\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m json_out:\n\u001b[1;32m    485\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m json\u001b[38;5;241m.\u001b[39mloads(contents)\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.12/site-packages/decorator.py:224\u001b[0m, in \u001b[0;36mdecorate.<locals>.fun\u001b[0;34m(*args, **kw)\u001b[0m\n\u001b[1;32m    222\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kwsyntax:\n\u001b[1;32m    223\u001b[0m     args, kw \u001b[38;5;241m=\u001b[39m fix(args, kw, sig)\n\u001b[0;32m--> 224\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m caller(func, \u001b[38;5;241m*\u001b[39m(extras \u001b[38;5;241m+\u001b[39m args), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.12/site-packages/gcsfs/retry.py:135\u001b[0m, in \u001b[0;36mretry_request\u001b[0;34m(func, retries, *args, **kwargs)\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m retry \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    134\u001b[0m         \u001b[38;5;28;01mawait\u001b[39;00m asyncio\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;28mmin\u001b[39m(random\u001b[38;5;241m.\u001b[39mrandom() \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m (retry \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m), \u001b[38;5;241m32\u001b[39m))\n\u001b[0;32m--> 135\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    136\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\n\u001b[1;32m    137\u001b[0m     HttpError,\n\u001b[1;32m    138\u001b[0m     requests\u001b[38;5;241m.\u001b[39mexceptions\u001b[38;5;241m.\u001b[39mRequestException,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    141\u001b[0m     aiohttp\u001b[38;5;241m.\u001b[39mclient_exceptions\u001b[38;5;241m.\u001b[39mClientError,\n\u001b[1;32m    142\u001b[0m ) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    143\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    144\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(e, HttpError)\n\u001b[1;32m    145\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m e\u001b[38;5;241m.\u001b[39mcode \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m400\u001b[39m\n\u001b[1;32m    146\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrequester pays\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m e\u001b[38;5;241m.\u001b[39mmessage\n\u001b[1;32m    147\u001b[0m     ):\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.12/site-packages/gcsfs/core.py:474\u001b[0m, in \u001b[0;36mGCSFileSystem._request\u001b[0;34m(self, method, path, headers, json, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m    471\u001b[0m info \u001b[38;5;241m=\u001b[39m r\u001b[38;5;241m.\u001b[39mrequest_info  \u001b[38;5;66;03m# for debug only\u001b[39;00m\n\u001b[1;32m    472\u001b[0m contents \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m r\u001b[38;5;241m.\u001b[39mread()\n\u001b[0;32m--> 474\u001b[0m \u001b[43mvalidate_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstatus\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontents\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    475\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m status, headers, info, contents\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.12/site-packages/gcsfs/retry.py:98\u001b[0m, in \u001b[0;36mvalidate_response\u001b[0;34m(status, content, path, args)\u001b[0m\n\u001b[1;32m     96\u001b[0m     path \u001b[38;5;241m=\u001b[39m path\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;241m*\u001b[39m[quote(p) \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m args])\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m status \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m404\u001b[39m:\n\u001b[0;32m---> 98\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(path)\n\u001b[1;32m    100\u001b[0m error \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    101\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(content, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdecode\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: b/leap-persistent/o/mauriekeppens%2FEnsemble_Testbed%2F02_ML_results%2FXGBOOST%2Fnmse%2Fmetrics%2Fnmse_best_params_dict_198202-202312_CESM2.pickle"
     ]
    }
   ],
   "source": [
    "### \"k_folds\" means number of times to do cross validation. \n",
    "### When k_folds = 3, the same parameters are run through grid search 3 times, optimizing for your chosen approach (nmse or bias).\n",
    "### The three scores are then averaged to get the overall score/performance for that combination of hyperparameters.\n",
    "\n",
    "# ----------------- Cross-validation setup -----------------\n",
    "k_folds = 3\n",
    "\n",
    "print(datetime.datetime.now())\n",
    "\n",
    "# ----------- Loop through ensembles and members --------------\n",
    "\n",
    "for ens, mem_list in mems_dict.items():\n",
    "    \n",
    "    # If \"first_mem = True\", grid search will run for one member of each Earth System Model in the testbed.\n",
    "    # If \"first_mem = False\", grid search will not run at all. \n",
    "        # Hyperparameters are then either: specified above in this notebook, or \n",
    "        # retrieved from a previously-made pickle file resulting from grid search.\n",
    "    first_mem = False \n",
    "\n",
    "    for member in mem_list:\n",
    "        print(ens,member)\n",
    "        \n",
    "        seed_loc = seed_loc_dict[ens][member]\n",
    "\n",
    "        # ----------- Load ML input data  ------------------\n",
    "        ### ML input path, a dataframe made in notebook 01 ###\n",
    "        data_dir = f\"{MLinputs_path_abby}/{ens}/{member}\" # select the dataframe that was made for that member of the ESM\n",
    "        fname = f\"MLinput_{ens}_{member.split('_')[-1]}_mon_1x1_{init_date}_{fin_date}.pkl\" # the name that was given to the dataframe made in notebook 01\n",
    "        file_path = f\"{data_dir}/{fname}\"\n",
    "\n",
    "        ### Read in ML input dataframe, create some selection filters, produce a reduced dataframe ###   \n",
    "        with fs.open(file_path,'rb') as filee:\n",
    "            df = pd.read_pickle(filee)\n",
    "            df['year'] = df.index.get_level_values('time').year\n",
    "            df['mon'] = df.index.get_level_values('time').month\n",
    "            df['year_month'] = df['year'].astype(str) + \"-\" + df['mon'].astype(str)\n",
    "            \n",
    "            print(f\"\\nEnsemble: {ens}, Member: {member}\")\n",
    "            print(\"DataFrame shape:\", df.shape)\n",
    "            print(\"Columns:\", df.columns)\n",
    "            print(\"First rows:\\n\", df.head())\n",
    "\n",
    "            # ----------- Filter data ------------------\n",
    "            recon_sel = (~df[features_sel+target_sel+['net_mask']].isna().any(axis=1)) & ((df[target_sel] < 250) & (df[target_sel] > -250)).to_numpy().ravel()\n",
    "                        # Select all rows where none of the chosen feature, target, or net_mask columns are missing.\n",
    "                        # filtered to only use pCO2 RESIDUAL values between -250 and 250 microatm \n",
    "                        # this more or less corresponds to filtering OUT values of pCO2 that are above 800 microatm, and therefore not realistic (remove the extreme outliers)\n",
    "                        # Positive residual â†’ pCOâ‚‚ is higher than expected from temperature alone\n",
    "                        # Negative residual â†’ pCOâ‚‚ is lower than expected from temperature alone\n",
    "            sel = (recon_sel & ((df['socat_mask'] == 1)))\n",
    "                        #  selects pCO2-residual values that correspond to SOCAT locations, \n",
    "\n",
    "            print(\"Total rows after filter the data:\", sum(recon_sel))\n",
    "            print(\"Number of filtered points corresponding to SOCAT:\", sum(sel))\n",
    "\n",
    "            # recon_sel -> has pCO2 + input variable data + in between the target_sel range + ocean\n",
    "            # sel -> recon_sel that corresponds to a real SOCAT observation point\n",
    "\n",
    "            # ----------- Split into train/test/unseen ------------------\n",
    "            ### selection of training data, using filtered selection above, following train/test selection in cell above\n",
    "            train_sel = (sel & (pd.Series(df['year_month']).isin(year_mon))).to_numpy().ravel() # year_mon is the selection train-val\n",
    "            print('Number of training points:',sum(train_sel)) \n",
    "\n",
    "            ### selection of test data, using filtered selection above, following train/test selection in cell above\n",
    "            test_sel = (sel & (pd.Series(df['year_month']).isin(test_year_mon))).to_numpy().ravel() # test_year_mon is the selection test\n",
    "            print('Number of testing points:',sum(test_sel))\n",
    "\n",
    "            ### selection of \"unseen\" data, corresponding to spatiotemporal points NOT in SOCAT (everything a \"0\" in the SOCAT mask)\n",
    "            unseen_sel = (recon_sel & (df['socat_mask'] == 0)) \n",
    "                            # the ensemble members give much more \"ground-truth\" pCO2 \"observations\" (ESM member pCO2 field) than the SOCAT coverage. \n",
    "                            # We use SOCAT mask to represent the \"real-world\" -> but the ESM member pCO2 field covers much more pCO2 values -> we can use that to extra validate the ML\n",
    "            print('Number of unseen points:',sum(unseen_sel)) \n",
    "\n",
    "            # ----------- Convert to NumPy arrays ------------------\n",
    "            ### Convert dataframe of training and test data to numpy arrays\n",
    "            X = df.loc[sel,features_sel].to_numpy()\n",
    "            y = df.loc[sel,target_sel].to_numpy().ravel()\n",
    "\n",
    "            ### Selects features and target variables from the training set\n",
    "            Xtrain = df.loc[train_sel,features_sel].to_numpy()                \n",
    "            ytrain = df.loc[train_sel,target_sel].to_numpy().ravel()\n",
    "\n",
    "            ### Selects features and target variables from the test set\n",
    "            X_test = df.loc[test_sel,features_sel].to_numpy()\n",
    "            y_test = df.loc[test_sel,target_sel].to_numpy().ravel()  \n",
    "\n",
    "            print(\"Xtrain shape:\", Xtrain.shape)\n",
    "            print(\"ytrain shape:\", ytrain.shape)\n",
    "            print(\"X_test shape:\", X_test.shape)\n",
    "            print(\"y_test shape:\", y_test.shape)\n",
    "\n",
    "            # --------------- Randomized splits -----------------------\n",
    "            ### Applying splits in randomized order to training/test/validation sets using supporting python file (\"supporting_functions\")\n",
    "            N = Xtrain.shape[0]\n",
    "            train_val_idx, train_idx, val_idx, test_idx = supporting_functions.train_val_test_split(N, test_prop, val_prop, random_seeds, seed_loc)\n",
    "            X_train_val, X_train, X_val, X_test_tmp, y_train_val, y_train, y_val, y_test_tmp = supporting_functions.apply_splits(Xtrain, ytrain, train_val_idx, train_idx, val_idx, test_idx)   \n",
    "            # So this function basically randomly shuffles the data and assigns each sample to the appropriate set, but reproducibly using the seed.\n",
    "            # X_test_tmp is just the internal â€œpseudo-testâ€ within the training data â€” itâ€™s not used for the final evaluation.\n",
    "\n",
    "            # --------------- Grid Search (Hyperparameter optimization) -----------------------\n",
    "            ### GRID SEARCH STARTS HERE:\n",
    "            ### Will optimize according to specified approach (bias or nmse)\n",
    "            if first_mem: # This grid seach is Only run for the first member (first_mem = True) to save time.\n",
    "                model = XGBRegressor(random_state=random_seeds[4,seed_loc], n_jobs=jobs)\n",
    "                param_grid = xg_param_grid\n",
    "\n",
    "                ### bias optimization\n",
    "                if grid_search_approach == 'bias':\n",
    "                    grid = GridSearchCV(model,  # This is the base machine learning model you want to optimize.\n",
    "                                        param_grid,  # A dictionary where keys = hyperparameter names, values = lists of possible values.\n",
    "                                        scoring=bias_scorer,  # Determines how to measure model performance.\n",
    "                                        cv=k_folds,  # Number of cross-validation folds.  (builded in the GridSearchCV function)\n",
    "                                                    # Each hyperparameter combination is evaluated 3 times (once per fold) and the scores are averaged.\n",
    "                                                    # After the 3 folds, every observation (train_val dataset) has been in the validation set exactly once.\n",
    "                                        return_train_score=True,  # Keeps the training score for each hyperparameter combination.\n",
    "                                        refit=False,  # Normally, after finding the best hyperparameters, GridSearchCV retrains the model on the whole training set. \n",
    "                                                      # refit=False disables that because maybe you just want the hyperparameter evaluation and will retrain manually later.\n",
    "                            verbose=3)\n",
    "                    \n",
    "                ### nmse optimization\n",
    "                elif grid_search_approach == 'nmse':\n",
    "                    grid = GridSearchCV(model, param_grid, scoring='neg_mean_squared_error', cv=k_folds, return_train_score=True, refit=False, \n",
    "                                    verbose=3)\n",
    "                print(grid)\n",
    "                ### Grid search here, using features and target variables from both training and validation sets, \n",
    "                    ### so should be using 80% of SOCAT sampling (60% train + 20% validation)\n",
    "                grid.fit(X_train_val, y_train_val) # using the training+validation data -> cv function within the GridSearchCV will split it in 3 parts\n",
    "                best_params = pd.DataFrame([grid.best_params_]) # best parameters chosen by grid search function due to best score\n",
    "                print(best_params)\n",
    "                scores = pd.DataFrame(grid.cv_results_)\n",
    "\n",
    "                ### save grid search scores\n",
    "                output_file = f\"CVgrid_scores_{ens}_{member}.csv\"\n",
    "                scores.to_csv(f'{metrics_output_dir}/{output_file}')\n",
    "                \n",
    "                ### save the best parameters\n",
    "                param_fname = f\"{metrics_output_dir}/{grid_search_approach}_best_params_dict_{init_date}-{fin_date}_{ens}.pickle\"\n",
    "\n",
    "                ### saving grid search best parameters in pickle file\n",
    "                best_params.to_pickle(param_fname)\n",
    "\n",
    "                ### Here, \"first_mem\" is set to false so grid search stops after one member per ESM\n",
    "                ### Do not change this to True UNLESS you want to do grid search on every member of the testbed\n",
    "                first_mem = False\n",
    "            ###### GRID SEARCH ENDS HERE ######\n",
    "\n",
    "            # --------------- Train ML model with best hyperparameters -----------------------\n",
    "            ### make blank dictionaries for performance metrics for training/testing/unseen sets\n",
    "            train_performance = defaultdict(dict)\n",
    "            test_performance = defaultdict(dict)\n",
    "            unseen_performance = defaultdict(dict)\n",
    "\n",
    "            ### loading best hyperparameters from up above in notebook, if hyperparameters are hard-coded in notebook\n",
    "            if best_params_dict is not None:\n",
    "                best_params = best_params_dict\n",
    "\n",
    "            ### loading previously-saved best hyperparameters from pickle file post-grid search\n",
    "            elif best_params_dict is None:\n",
    "                best_params_path = f\"{metrics_output_dir}/{grid_search_approach}_best_params_dict_{init_date}-{fin_date}_{ens}.pickle\"\n",
    "                print(best_params_path)\n",
    "                best_params = pd.read_pickle(best_params_path).to_dict('records')[0]\n",
    "                print(\"Best hyperparameters:\", best_params)\n",
    "\n",
    "            ### Running ML using hyperparameters: setting up ML model and then training it (\".fit\") on training set\n",
    "            model = XGBRegressor(random_state=random_seeds[5,seed_loc], **best_params, n_jobs=jobs)\n",
    "                        # the n_jobs parameter controls parallelism, i.e., how many CPU cores the model can use when training.\n",
    "            model.fit(X_train_val, y_train_val, verbose=True) # Trains the final XGBoost model using training + validation data.   \n",
    "            ### Save the resulting ML model\n",
    "            #supporting_functions.save_model(model, dates, xgb_model_save_dir, ens, member) ## this is to save model, in progress \n",
    "\n",
    "            # --------------- Evaluate performance: test dataset -----------------------\n",
    "            ### Makes ML prediction/reconstruction on test set ML model to test set (\".predict\" and \".evaluate_test\"), calculate test error metrics and store in a dictionary.\n",
    "            ### X_test is the feature variables in the test set.\n",
    "            ### Using the test set feature variables and the algorithm produced using the training data, \n",
    "                ### pCO2-residual (the target) is predicted.\n",
    "            ### The predicted pCO2-residual within the test set domain is now compared to the TRUE test set pCO2-residual values.\n",
    "            ### The result is the test performance.\n",
    "\n",
    "            y_pred_test = model.predict(X_test)\n",
    "\n",
    "            print(\"y_test length:\", len(y_test))\n",
    "            print(\"y_pred_test length:\", len(y_pred_test))\n",
    "\n",
    "            try:\n",
    "                test_performance[ens][member] = supporting_functions.evaluate_test(y_test, y_pred_test)\n",
    "                                        # This is a dictionary that stores the performance metrics of your ML model for a specific ensemble member.\n",
    "            except Exception as e:\n",
    "                print(f\"Error evaluating {ens} {member}: {e}\")\n",
    "\n",
    "            ### This saves the test performance metrics for every member in the testbed ###\n",
    "            fields = test_performance[ens][member].keys()\n",
    "            test_row_dict = dict()\n",
    "            test_row_dict['model'] = ens # Which ESM \n",
    "            test_row_dict['member'] = member # Which ESM member\n",
    "            \n",
    "            for field in fields: # returns all the metric names (the â€œfieldsâ€) in that dictionary. for example: rmse, bias, r2\n",
    "                test_row_dict[field] = test_performance[ens][member][field] \n",
    "\n",
    "            with fs.open(test_perform_fname, 'a') as f_object:\n",
    "                writer = csv.DictWriter(f_object, fieldnames = test_row_dict.keys())\n",
    "                if not fs.exists(test_perform_fname):\n",
    "                    writer.writeheader() \n",
    "                writer.writerow(test_row_dict)\n",
    "\n",
    "            print('test performance metrics:', test_performance[ens][member])\n",
    "            \n",
    "            ##############\n",
    "\n",
    "            # --------------- Evaluate performance: no-SOCAT dataset -----------------------\n",
    "            ### Global reconstruction where we don't have SOCAT data\n",
    "            ### Then compares reconstruction to the testbed truth, which in our case is the pCO2-residual calculated from ESM output\n",
    "            y_pred_unseen = model.predict(df.loc[unseen_sel,features_sel].to_numpy())\n",
    "            y_unseen = df.loc[unseen_sel,target_sel].to_numpy().ravel()\n",
    "            unseen_performance[ens][member] = supporting_functions.evaluate_test(y_unseen, y_pred_unseen)\n",
    "\n",
    "            ### This saves the UNSEEN performance metrics for every member in the testbed ###\n",
    "            fields = unseen_performance[ens][member].keys()\n",
    "            unseen_row_dict = dict()\n",
    "            unseen_row_dict['model'] = ens\n",
    "            unseen_row_dict['member'] = member\n",
    "            \n",
    "            for field in fields:\n",
    "                unseen_row_dict[field] = unseen_performance[ens][member][field]\n",
    "\n",
    "            with fs.open(unseen_perform_fname, 'a') as f_object:\n",
    "                writer = csv.DictWriter(f_object, fieldnames = unseen_row_dict.keys())\n",
    "                if not fs.exists(unseen_perform_fname):\n",
    "                    writer.writeheader() \n",
    "                writer.writerow(unseen_row_dict)\n",
    "\n",
    "            print('unseen performance metrics:', unseen_performance[ens][member])\n",
    "            \n",
    "            ##############\n",
    "            \n",
    "            # ML model predicts using feature variables from within SOCAT domain (should be all training and test locations/times)\n",
    "            # This is in addition to the unseen prediction that's performed above\n",
    "            y_pred_seen = model.predict(X) # X contains features for all SOCAT-covered points (train + test).\n",
    "\n",
    "            # Full reconstruction globally, every grid cell of testbed #\n",
    "            # Unseen and \"seen\" (train + test) reconstructions are combined\n",
    "            df['pCO2_recon_full'] = np.nan # Creates a new column for full global reconstruction.\n",
    "            df.loc[unseen_sel,['pCO2_recon_full']] = y_pred_unseen  # Fills unseen areas (unseen_sel) with predictions from the unseen domain.\n",
    "            df.loc[sel,['pCO2_recon_full']] = y_pred_seen # Fills SOCAT-covered areas (sel) with predictions from the seen domain.\n",
    "            \n",
    "            # Reconstruction in times/locations of TEST set domain, done by making all TRAINING points into nans\n",
    "            df['pCO2_recon_test'] = np.nan\n",
    "            df.loc[unseen_sel,['pCO2_recon_test']] = np.nan\n",
    "            df.loc[sel,['pCO2_recon_test']] = y_pred_seen # Column pCO2_recon_test is filled in with the predictions for SOCAT POINTs\n",
    "            df.loc[train_sel, ['pCO2_recon_test']] = np.nan # The training points are put back to nan values -> we only want to see the test predictions\n",
    "\n",
    "            # Reconstructed in times/locations of TRAINING set domain, done by making all TESTING points into nans\n",
    "            df['pCO2_recon_train'] = np.nan\n",
    "            df.loc[unseen_sel,['pCO2_recon_train']] = np.nan\n",
    "            df.loc[sel,['pCO2_recon_train']] = y_pred_seen \n",
    "            df.loc[test_sel, ['pCO2_recon_train']] = np.nan # Column pCO2_recon_train shows predictions only for training points in SOCAT, test points are put to nan values again\n",
    "        \n",
    "            # Reconstruction everywhere outside of SOCAT domain (this is what we call \"unseen\")\n",
    "            df['pCO2_recon_unseen'] = np.nan\n",
    "            df.loc[unseen_sel,['pCO2_recon_unseen']] = y_pred_unseen # pCO2_recon_unseen contains predictions only where SOCAT data is not available.\n",
    "            df.loc[sel,['pCO2_recon_unseen']] = np.nan\n",
    "            \n",
    "            # Reconstruction at time/locations of SOCAT sampling (train + test)\n",
    "            df['pCO2_recon_socat'] = np.nan\n",
    "            df.loc[unseen_sel,['pCO2_recon_socat']] = np.nan\n",
    "            df.loc[sel,['pCO2_recon_socat']] = y_pred_seen # This isolates predictions at all SOCAT-covered points, including both train and test.\n",
    "\n",
    "            # Testbed truth (pco2 residual calculated from ESM output)\n",
    "            df['pCO2_residual_testbed_truth'] = df['pco2_residual'] # This column serves as the ground truth from the ESM output.\n",
    "\n",
    "            DS_recon = df[['net_mask','socat_mask','pCO2_residual_testbed_truth','pCO2_recon_full','pCO2_recon_socat','pCO2_recon_unseen', 'pCO2_recon_test', 'pCO2_recon_train']].to_xarray()\n",
    "\n",
    "            supporting_functions.save_recon(DS_recon, dates, recon_output_dir, ens, member)\n",
    "            print('end of member', datetime.datetime.now())\n",
    "            \n",
    "print('end of all members', datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25c6a7eb-3e77-4187-a43e-1cac54bb5052",
   "metadata": {},
   "source": [
    "<span style=\"color:hotpink; font-size:40px; font-weight:bold;\">To view ranking of hyperparameters and their various scores</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa24d1c1-9a14-4e3e-9cbf-bd4a19efb288",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# scoresCV = scores[['params', 'mean_test_score', 'std_test_score', 'mean_train_score']].sort_values(by = 'mean_test_score' \\\n",
    "#                    , ascending = False) #mean = averaged over the k folds (in this case, the 3 partitions of the cross validation); std = standard deviation\n",
    "\n",
    "# scoresCV = scores[['params', 'mean_test_score', 'std_test_score', 'mean_fit_time']].sort_values(by = 'mean_test_score' \\\n",
    "#                     , ascending = False) #mean = averaged over the k folds (in this case, the 3 partitions of the cross validation); std = standard deviation\n",
    "\n",
    "# scoresCV.head(10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
