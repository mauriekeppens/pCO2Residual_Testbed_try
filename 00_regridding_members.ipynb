{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3d5c1892-fd91-4483-a420-713bbcacfa64",
   "metadata": {},
   "source": [
    "<span style=\"color:hotpink; font-size:40px; font-weight:bold;\">Packages and imports</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6318411e-1e93-45da-b76d-e4bf84c7dcbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xmip.preprocessing import combined_preprocessing\n",
    "from xmip.postprocessing import concat_experiments, merge_variables\n",
    "from xmip.utils import cmip6_dataset_id\n",
    "\n",
    "from dask.diagnostics import ProgressBar\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import xesmf as xe\n",
    "import pandas as pd\n",
    "import intake\n",
    "\n",
    "import gcsfs\n",
    "fs = gcsfs.GCSFileSystem()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83a8b833-dfdc-44ea-9f6c-83857407bda5",
   "metadata": {},
   "source": [
    "<span style=\"color:hotpink; font-size:40px; font-weight:bold;\">Setting path for saving cleaned CMIP6 testbed files</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df79f24f-da3f-4465-920d-90221f6ffc73",
   "metadata": {},
   "outputs": [],
   "source": [
    "### set paths ###\n",
    "\n",
    "ensemble_dir = 'gs://leap-persistent/abbysh/pco2_all_members_1982-2023/post00_regridded_members'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "140c4ccd-912a-4af4-ac3b-d91f1fae3a34",
   "metadata": {},
   "source": [
    "<span style=\"color:hotpink; font-size:40px; font-weight:bold;\">Load CMIP6 datasets from cloud that satisfy our requirements</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbb60eac-e829-463f-84e4-c9f3104564d9",
   "metadata": {},
   "source": [
    "<span style=\"color:lightblue; font-size:30px; font-weight:bold;\">CMIP6 Earth System Models (ESMs) we want, called \"source_id\"</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea8726a1-2b13-45dd-ad28-5a7589482769",
   "metadata": {},
   "outputs": [],
   "source": [
    "source_id=['ACCESS-ESM1-5','UKESM1-0-LL','CMCC-ESM2','CESM2-WACCM','CESM2','CanESM5-CanOE','CanESM5','MPI-ESM1-2-LR','GFDL-ESM4']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ac19933-fbe1-4ca5-a737-fdb7fd77c6e8",
   "metadata": {},
   "source": [
    "<span style=\"color:lightblue; font-size:30px; font-weight:bold;\">Searching CMIP6 catalog</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "846741ff-019d-487c-870b-09702dc2e44d",
   "metadata": {},
   "source": [
    "<span style=\"color:thistle; font-size:25px; font-weight:bold;\">Definitions and units</span>\n",
    "\n",
    "Note: To convert pascals to microatm: 1 atmosphere (atm) = 101325 pascal (Pa) and then 10^6uatm = 1atm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4b0966c-48b0-4682-b682-bc0152cb146a",
   "metadata": {},
   "source": [
    "| Our variable name | CMIP6 output name  | Description                      | Units                                  |\n",
    "|-------------------|--------------------|----------------------------------|----------------------------------------|\n",
    "| pCO2              | spco2              | sea surface co2 partial pressure |pascals, CONVERTS TO MICROATM LATER     |\n",
    "| SST               | tos                | sea surface temperature          |degrees Celsius                         |\n",
    "| SSS               | sos                | sea surface salinity             |.001 (parts per thousand)               |\n",
    "| Chl               | chl                | sea surface chlorophyll          |kilograms per cubic meter               |\n",
    "| MLD               | mlotst             | mixed layer depth                |meters (defined by sigma T criterion)   |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "074ce676-612f-414f-8456-ad843a1b064e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter the full catalog for data we could use\n",
    "\n",
    "# This is the store for CMIP6 datasets that pass ingestion tests. For more information: https://github.com/leap-stc/cmip6-leap-feedstock\n",
    "url = \"https://storage.googleapis.com/cmip6/cmip6-pgf-ingestion-test/catalog/catalog.json\"\n",
    "col = intake.open_esm_datastore(url)\n",
    "\n",
    "##search for data##\n",
    "cat = col.search(\n",
    "    source_id = source_id, # ESM list\n",
    "    variable_id=['tos', 'sos', 'chl', 'mlotst', 'spco2'], #variables we want, descriptions written above\n",
    "    table_id=['Omon'], # monthly ocean output only\n",
    "    experiment_id=['ssp245','historical'], # ssp scenario of choice, plus historical for pre-2014 model output\n",
    "    require_all_on=['source_id', 'member_id', 'grid_label'] # this ensures that results will have all variables and experiments available\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8066e54a-fb27-4878-b653-2c5d59971370",
   "metadata": {},
   "source": [
    "<span style=\"color:thistle; font-size:25px; font-weight:bold;\">To view available datasets, number of members per ESM</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70c95f84-ba04-4668-9619-49bc9100eb87",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat.df.groupby(['source_id', 'grid_label','table_id'])[['member_id']].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64031e33-66bc-4157-8474-a359407c206f",
   "metadata": {},
   "source": [
    "<span style=\"color:hotpink; font-size:40px; font-weight:bold;\">Regridding</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3005ef8a-4512-49e0-9284-61cbd9130e4b",
   "metadata": {},
   "source": [
    "<span style=\"color:lightblue; font-size:30px; font-weight:bold;\">Turn catalog of ESM datasets into dictionary</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c57307e-88e3-47c2-b80e-8e4f2264dee4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "##turn data into dataset dictionary##\n",
    "ddict = cat.to_dataset_dict(\n",
    "    preprocess=combined_preprocessing,\n",
    "    xarray_open_kwargs=dict(use_cftime=True),\n",
    "    aggregate=False\n",
    ")\n",
    "\n",
    "## some arrays have an \"area\" variable. This is to drop that variable:\n",
    "\n",
    "for item in ddict:\n",
    "    if 'area' in ddict[item]:\n",
    "        ddict[item] = ddict[item].drop_vars('area')\n",
    "\n",
    "## ignore the warnings!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9dcac79-c9d7-4bc7-b90a-424cbc9505bd",
   "metadata": {},
   "source": [
    "<span style=\"color:lightblue; font-size:30px; font-weight:bold;\">Filltering out members with buggy times</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e46c4556-e142-4a88-a963-382c700c6294",
   "metadata": {},
   "source": [
    "What the following hack does is take the \"time\" data from a historical member, and a scenario member, that have no bugs.\n",
    "\n",
    "We then apply the non-buggy historical \"time\" data to all the historical members, and apply the non-buggy scenario \"time\" data to all the scenario members. \n",
    "\n",
    "If a member does not satisfy the necessary requirements (historical \"time\" data starts in 1850 and ends in 2014, for example), it is removed from our list of members."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1e9a233-aa8f-495a-9c4c-410b25a5221d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## temporary time hack due to time bugs ###\n",
    "\n",
    "hist_time = ddict['CMIP.CCCma.CanESM5.historical.r6i1p1f1.Omon.sos.gn.none.r6i1p1f1.v20190429.gs://cmip6/CMIP6/CMIP/CCCma/CanESM5/historical/r6i1p1f1/Omon/sos/gn/v20190429/'].time\n",
    "fut_time = ddict['ScenarioMIP.CCCma.CanESM5.ssp245.r3i1p1f1.Omon.sos.gn.none.r3i1p1f1.v20190429.gs://cmip6/CMIP6/ScenarioMIP/CCCma/CanESM5/ssp245/r3i1p1f1/Omon/sos/gn/v20190429/'].time\n",
    "dict_list = list(ddict.values())\n",
    "for item in dict_list:\n",
    "    if item.time.data[0].year == 1850 and item.time.data[-1].year == 2014:\n",
    "        item['time'] = hist_time\n",
    "    elif item.time.data[0].year == 2015 and item.time.data[-1].year == 2100:\n",
    "        item['time'] = fut_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c95f3323-e61d-404b-862c-f974215a791d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## functions like xarray.concat: chains together historical + scenario for each member, so time span is 1850-2100\n",
    "\n",
    "ds = concat_experiments(ddict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00f6339c-e222-4ceb-875a-5a940affff10",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## functions like xarray.merge: combines separate variables into one dataset per member\n",
    "\n",
    "ds = merge_variables(ds) \n",
    "\n",
    "# lots of warnings right now, but should be fine! the warnings mean the bad members are getting removed\n",
    "# TODO: I think we can fix this bug by doing a time slice for when we want on each dataset BEFORE merging"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64569f30-6501-45c4-9237-67b1569b9cc7",
   "metadata": {},
   "source": [
    "<span style=\"color:lightblue; font-size:30px; font-weight:bold;\">Setting up target grid</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16578a5e-10d1-4ce2-89c6-41c24af847e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## create desired resolution and time frame \n",
    "# This is set up for -180 thru 180 degrees for longitude, -90 thru 90 degrees for latitude \n",
    "# ^ As opposed to 0 thru 360 for longitude, for example\n",
    "\n",
    "ylat = xr.DataArray(data=[x+.5 for x in range(-90, 90, 1)], dims=['ylat'], coords=dict( ylat=(['ylat'],[x+.5 for x in range(-90, 90, 1)]) ),)\n",
    "xlon = xr.DataArray(data=[x+.5 for x in range(-180,180,1)], dims=['xlon'], coords=dict( xlon=(['xlon'],[x+.5 for x in range(-180,180,1)]) ),)\n",
    "# alternatively: xlon = xr.DataArray(data=[x+.5 for x in range(0,360,1)], dims=['xlon'], coords=dict( xlon=(['xlon'],[x+.5 for x in range(0,360,1)]) ),)\n",
    "\n",
    "## desired start and end 'year-month' for testbed \n",
    "processed_start_yearmonth = '1982-02'\n",
    "processed_end_yearmonth = '2023-12'\n",
    "\n",
    "## desired start and end year for testbed\n",
    "init_year = 1982\n",
    "fin_year = 2023\n",
    "\n",
    "## time should be monthly on the middle of the month ('freq = \"MS\") refers to \"month start\" frequency, and we add 14 days to get to mid-month\n",
    "# note that the time doesnt affect regridding but we do use this time to overwrite the monthly dates so its consistent\n",
    "ttime = pd.date_range(start=str(processed_start_yearmonth), end=str(processed_end_yearmonth),freq='MS') + np.timedelta64(14, 'D') \n",
    "\n",
    "## set up our desired grid. It must be named this way for old XESFM versions\n",
    "target_grid = xr.Dataset({'time':(['time'],ttime.values), 'latitude':(['latitude'],ylat.values),'longitude':(['longitude'],xlon.values)}) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6b5010f-c94a-4666-af4f-f8bf7573cd1a",
   "metadata": {},
   "source": [
    "<span style=\"color:lightblue; font-size:30px; font-weight:bold;\">Functions for regridding data</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be3d2870-c2ce-46c7-bd90-b3708f7f1dce",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def replace_calendar(ds:xr.Dataset) -> xr.Dataset:\n",
    "    \"\"\"\n",
    "    Sets new time data for xarray dataset, according to target grid defined above.\n",
    "\n",
    "    Args:\n",
    "        ds (xr.Dataset): Initial dataset with times to be fixed.\n",
    "    Returns: \n",
    "        ds (xr.Dataset): Dataset with fixed times.\n",
    "    \"\"\"\n",
    "    year = ds.time.data[0].year\n",
    "    month = ds.time.data[0].month\n",
    "    start_date = f'{year}-{month:0>2}-01'\n",
    "    new_monthly_time = xr.cftime_range(start_date, periods=len(ds.time), freq='1MS')\n",
    "    ds = ds.assign_coords(time=new_monthly_time)\n",
    "    return ds\n",
    "\n",
    "#TODO:  create a regridder dict per source_id (faster)\n",
    "\n",
    "# target_grid = xe.util.grid_global(1,1) # this is old, from julius\n",
    "def regrid(target_grid, ds:xr.Dataset) -> xr.Dataset:\n",
    "    \"\"\"\n",
    "    Regrids dataset to match times/lats/lons we want.\n",
    "\n",
    "    Args:\n",
    "        target_grid (xr.Dataset): Ideal dataset example in terms of format of times/lat/lon.\n",
    "        ds (xr.Dataset): Dataset to regrid to match format of target_grid.\n",
    "    Returns:\n",
    "        ds_regridded (xr.Dataset): Dataset changed to match target_grid format.\n",
    "    \"\"\"\n",
    "    #FIXME: This should not be done for every dataset\n",
    "    regridder = xe.Regridder(ds, target_grid, 'bilinear', ignore_degenerate=True, periodic=True) \n",
    "    #TODO: Check if this should be conservative?\n",
    "    ds_regridded = regridder(ds, keep_attrs=True)\n",
    "    \n",
    "    return ds_regridded\n",
    "\n",
    "def full_testbed_processing(target_grid, init_year, fin_year, timespan, ds:xr.Dataset) -> xr.Dataset:\n",
    "    \"\"\"\n",
    "\n",
    "    Args:\n",
    "        target_grid (xr.Dataset): Ideal dataset example in terms of format of times/lat/lon.\n",
    "        init_year (int): Initial year included in dataset.\n",
    "        fin_year (int): Final year included in dataset.\n",
    "        timespan (int): Number of months in dataset (time length of dataset).\n",
    "        ds (xr.Dataset): Dataset to regrid to match format of target_grid.\n",
    "\n",
    "    Returns:\n",
    "        ds_new_cal (xr.Dataset): Dataset gridded to match target_grid format, with changed calendar and removed extraneous variables.\n",
    "    \"\"\"\n",
    "    ds = ds.squeeze(drop=True)\n",
    "    # select surface depth (for chl, TODO: Check if surface chlorophyll is available)\n",
    "    if 'lev' in ds.dims:\n",
    "        ds = ds.isel(lev=0).drop_vars(('lev'))\n",
    "    \n",
    "    ds = ds.sel(time=slice(str(init_year),str(fin_year)))\n",
    "    # testing\n",
    "    assert len(ds.time) == timespan\n",
    "    assert ds.time.data[0].year == int(init_year[0:4])\n",
    "    \n",
    "    # Processing\n",
    "    ds_regridded = regrid(target_grid, ds)\n",
    "    ds_new_cal = replace_calendar(ds_regridded)\n",
    "\n",
    "    return ds_new_cal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8378ddc1-076b-4fb1-8fcb-315066f912fe",
   "metadata": {},
   "source": [
    "<span style=\"color:lightblue; font-size:30px; font-weight:bold;\">Regridding all members and saving them</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a933728-40c5-4239-b475-c9c8dc882138",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "member_counter = 0\n",
    "\n",
    "## loop through all members\n",
    "\n",
    "for k,item in ds.items():\n",
    "        print(f\"Processing member no.{member_counter},{k}\")\n",
    "\n",
    "        ## regridding step here\n",
    "        item_out = full_testbed_processing(target_grid, processed_start_yearmonth, processed_end_yearmonth, len(ttime), item)\n",
    "\n",
    "        ## get CMIP6 ID for member\n",
    "        item_id = cmip6_dataset_id(item_out, id_attrs=[\n",
    "            'source_id',\n",
    "            'variant_label',\n",
    "            'table_id'\n",
    "        ])\n",
    "\n",
    "        ## converting pco2 to microatmospheres, from pascals!\n",
    "        if 'spco2' in list(item_out.keys()):\n",
    "            if item_out['spco2'].attrs['units'] == 'Pa':\n",
    "                print('fixing spco2 units')\n",
    "                new_spco2 = item_out.spco2 * 10**6 / 101325  #to get to microatm\n",
    "                item_out['spco2'].values = new_spco2\n",
    "                item_out['spco2'].attrs[\"units\"] = 'microatmospheres'\n",
    "        \n",
    "        ### removing unneccessary variables ###\n",
    "        if 'lev_bounds' in item_out:\n",
    "            item_out = item_out.drop_vars(('lev_bounds'))\n",
    "        if 'time_bounds' in item_out:\n",
    "            item_out = item_out.drop_vars(('time_bounds'))\n",
    "        if 'lev_partial' in item_out:\n",
    "            item_out = item_out.drop_vars(('lev_partial'))\n",
    "        if 'nbnd' in item_out:\n",
    "            item_out = item_out.drop_vars(('nbnd'))\n",
    "        if 'lev_partial' in item_out.chl.dims:\n",
    "            item_out['chl'] = item_out['chl'].sel({'lev_partial':1},drop=True)\n",
    "    \n",
    "        ## fixing variable/coord names ###\n",
    "        fixed_names = ['ylat','xlon','mld','sss','sst']\n",
    "        old_names = ['latitude','longitude','mlotst','sos','tos']\n",
    "        \n",
    "        for new,old in zip(fixed_names,old_names):\n",
    "            item_out = item_out.rename({old:new})\n",
    "        \n",
    "        ## calculate pco2-T (temperature component of pco2) and pco2-residual (or \"non-T pco2\")\n",
    "        pco2_T_calc = item_out['spco2'].mean('time') * np.exp(0.0423 * (item_out['sst'] - item_out['sst'].mean(\"time\")))\n",
    "        pco2_resid_calc = item_out['spco2'] - pco2_T_calc\n",
    "        \n",
    "        item_out = item_out.assign(pco2_T = pco2_T_calc)\n",
    "        item_out = item_out.assign(pco2_residual = pco2_resid_calc)\n",
    "\n",
    "        ## to make sure dimensions are lined up for all variables\n",
    "        item_out = item_out.transpose('time','ylat','xlon')\n",
    "\n",
    "        ## save regridded member data\n",
    "        save_path = f\"{ensemble_dir}/{item.attrs['source_id']}/member_{item.attrs['intake_esm_attrs:member_id']}/{item_id}.zarr\"\n",
    "        print(f\"Writing to {save_path = }\")\n",
    "        with ProgressBar():\n",
    "            item_out.chunk({'time':200}).to_zarr(save_path, mode='w')\n",
    "        member_counter +=1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
